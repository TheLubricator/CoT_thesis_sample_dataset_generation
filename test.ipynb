{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b862c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"enter gemini api key here\"\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# List available models to find one that supports generateContent\n",
    "# for m in genai.list_models():\n",
    "#   if 'generateContent' in m.supported_generation_methods:\n",
    "#     print(m.name)\n",
    "\n",
    "model = genai.GenerativeModel('models/gemini-2.5-flash-lite')  # Free, fast for CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c8854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Resuming from checkpoint: 2000 samples already completed\n",
      "📊 Progress: 2000/3000 (66.7%)\n",
      "🚀 Generating 1000 new samples...\n",
      "💾 Checkpoints every 20 samples\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   2%|▏         | 19/1000 [02:00<1:40:59,  6.18s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2020/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   4%|▍         | 39/1000 [04:06<1:39:41,  6.22s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2040/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   6%|▌         | 59/1000 [06:18<2:05:21,  7.99s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2060/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   8%|▊         | 79/1000 [08:25<1:37:29,  6.35s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2080/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  10%|▉         | 99/1000 [10:29<1:35:26,  6.36s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2100/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  12%|█▏        | 119/1000 [12:37<1:39:20,  6.77s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2120/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  14%|█▍        | 139/1000 [14:45<1:25:18,  5.95s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2140/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  16%|█▌        | 159/1000 [16:49<1:23:17,  5.94s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2160/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  18%|█▊        | 179/1000 [19:03<1:45:55,  7.74s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2180/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  20%|█▉        | 199/1000 [21:12<1:25:19,  6.39s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2200/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  22%|██▏       | 219/1000 [23:17<1:21:26,  6.26s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2220/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  24%|██▍       | 239/1000 [25:22<1:18:21,  6.18s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2240/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  26%|██▌       | 259/1000 [27:28<1:16:22,  6.18s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2260/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  28%|██▊       | 279/1000 [29:40<1:23:12,  6.92s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2280/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  30%|██▉       | 299/1000 [31:46<1:12:43,  6.23s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2300/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  32%|███▏      | 319/1000 [33:51<1:12:31,  6.39s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2320/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  34%|███▍      | 339/1000 [35:58<1:11:28,  6.49s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2340/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  36%|███▌      | 359/1000 [38:05<1:05:22,  6.12s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2360/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  38%|███▊      | 379/1000 [40:18<1:09:06,  6.68s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2380/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  40%|███▉      | 399/1000 [42:26<1:03:09,  6.31s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2400/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  42%|████▏     | 419/1000 [44:44<58:36,  6.05s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2420/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  44%|████▍     | 439/1000 [46:47<56:29,  6.04s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2440/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  46%|████▌     | 459/1000 [48:52<57:47,  6.41s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2460/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  48%|████▊     | 479/1000 [50:56<53:24,  6.15s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2480/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  50%|████▉     | 499/1000 [53:02<52:04,  6.24s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2500/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  52%|█████▏    | 519/1000 [55:13<51:13,  6.39s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2520/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  54%|█████▍    | 539/1000 [57:20<47:43,  6.21s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2540/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  56%|█████▌    | 559/1000 [59:22<45:18,  6.16s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2560/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  58%|█████▊    | 579/1000 [1:01:25<41:20,  5.89s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2580/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  60%|█████▉    | 599/1000 [1:03:30<41:24,  6.19s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2600/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  62%|██████▏   | 619/1000 [1:05:37<39:22,  6.20s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2620/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  64%|██████▍   | 639/1000 [1:07:46<38:34,  6.41s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2640/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  66%|██████▌   | 659/1000 [1:09:52<35:47,  6.30s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2660/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  68%|██████▊   | 679/1000 [1:12:10<34:40,  6.48s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2680/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  70%|██████▉   | 699/1000 [1:14:16<30:58,  6.17s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2700/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  72%|███████▏  | 719/1000 [1:16:20<29:38,  6.33s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2720/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  74%|███████▍  | 739/1000 [1:18:26<26:52,  6.18s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2740/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  76%|███████▌  | 759/1000 [1:20:37<25:17,  6.30s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2760/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  78%|███████▊  | 779/1000 [1:22:44<22:58,  6.24s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2780/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  80%|███████▉  | 799/1000 [1:24:51<22:31,  6.72s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2800/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  82%|████████▏ | 819/1000 [1:26:54<18:39,  6.19s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2820/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  84%|████████▍ | 839/1000 [1:29:04<17:11,  6.41s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2840/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  86%|████████▌ | 859/1000 [1:31:09<15:03,  6.41s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2860/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  88%|████████▊ | 879/1000 [1:33:12<12:38,  6.27s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2880/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  90%|████████▉ | 899/1000 [1:35:19<10:30,  6.24s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2900/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  92%|█████████▏| 919/1000 [1:37:30<08:26,  6.25s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2920/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  94%|█████████▍| 939/1000 [1:39:38<06:39,  6.55s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2940/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  96%|█████████▌| 959/1000 [1:41:45<04:12,  6.17s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2960/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  98%|█████████▊| 979/1000 [1:43:50<02:09,  6.16s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 2980/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|█████████▉| 995/1000 [1:45:32<00:30,  6.18s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating content for question: Amiyah is cutting some trees to build a cow shade. For every tree she cuts, she plants 5 new trees. If there were 400 trees on her farm and she cut 20% of them, calculate the total number of trees on the farm.\n",
      "Error details: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 1000\n",
      "Please retry in 26.784657949s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 1000\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 26\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|█████████▉| 999/1000 [1:45:55<00:06,  6.02s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint: 3000/3000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 1000/1000 [1:46:01<00:00,  6.36s/question]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generation complete! 3000 samples saved to cot_improved_gsm8k_checkpoint.json\n",
      "🏁 Final dataset: cot_improved_gsm8k_final_3000.json\n",
      "🏁 Final dataset: cot_improved_gsm8k_final_3000.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ds_gsm = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:6000]\")\n",
    "questions = [{\"q\": ex[\"question\"], \"gold\": ex[\"answer\"], \"domain\": \"gsm8k\"} for ex in ds_gsm]\n",
    "\n",
    "def generate_cot_improved(q, prompt_type=\"calculation_focused\"):\n",
    "    \"\"\"Enhanced CoT generation with better prompting for GSM8K\"\"\"\n",
    "    \n",
    "    if prompt_type == \"calculation_focused\":\n",
    "        prompt = f\"\"\"I need to solve this math problem carefully, paying special attention to calculations.\n",
    "\n",
    "Problem: {q['q']}\n",
    "\n",
    "I will:\n",
    "- Identify all given numbers\n",
    "- Determine what operations are needed  \n",
    "- Show each calculation step by step\n",
    "- Verify my arithmetic\n",
    "- Give a clear final answer\n",
    "\n",
    "Step-by-step solution:\"\"\"\n",
    "\n",
    "    elif prompt_type == \"self_verification\":\n",
    "        prompt = f\"\"\"Solve this math problem and then verify your answer.\n",
    "\n",
    "Problem: {q['q']}\n",
    "\n",
    "Solution process:\n",
    "1. Understanding: What am I solving for?\n",
    "2. Given information: What numbers and facts do I have?\n",
    "3. Step-by-step calculation:\n",
    "4. Verification: Does my answer make sense?\n",
    "5. Final answer:\n",
    "\n",
    "Let me solve this carefully:\"\"\"\n",
    "    \n",
    "    else:  # default - original approach\n",
    "        prompt = f\"Q: {q['q']}\\nLet's think step by step.\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        trace = response.text\n",
    "        \n",
    "        # IMPROVED regex patterns - prioritize \"last line with number\" approach\n",
    "        \n",
    "        # Method 1: Last line with number (most reliable) - FIXED regex ordering for better number capture\n",
    "        lines = trace.split('\\n')\n",
    "        last_line_number = None\n",
    "        for line in reversed(lines):\n",
    "            # FIXED regex: Prioritize longer numbers first, then comma-separated numbers\n",
    "            numbers = re.findall(r'(\\d+(?:,\\d{3})*|\\d{1,3}(?:,\\d{3})+)', line)  # \"8798\", \"3,500\" but not partial matches\n",
    "            if not numbers:\n",
    "                # Fallback: capture any sequence of digits (for cases without commas)\n",
    "                numbers = re.findall(r'(\\d+)', line)\n",
    "            \n",
    "            if numbers:\n",
    "                # Remove commas and convert to clean numbers for comparison\n",
    "                clean_numbers = [num.replace(',', '') for num in numbers]\n",
    "                \n",
    "                # IMPROVED SELECTION LOGIC: Prioritize the best number\n",
    "                if clean_numbers:\n",
    "                    # 1. If line contains final answer indicators, prioritize the largest number\n",
    "                    if any(indicator in line.lower() for indicator in ['final', 'answer', 'total', 'altogether', '####']):\n",
    "                        # Pick the largest number (most likely the final answer)\n",
    "                        last_line_number = max(clean_numbers, key=lambda x: int(x))\n",
    "                    else:\n",
    "                        # 2. For non-final lines, prioritize numbers >= 3 digits, then largest\n",
    "                        large_numbers = [n for n in clean_numbers if len(n) >= 3]\n",
    "                        if large_numbers:\n",
    "                            last_line_number = max(large_numbers, key=lambda x: int(x))  # Largest 3+ digit number\n",
    "                        else:\n",
    "                            last_line_number = max(clean_numbers, key=lambda x: int(x))  # Largest number overall\n",
    "                break\n",
    "        \n",
    "        # Method 2: Structured patterns as fallback - ENHANCED with better number matching\n",
    "        patterns = [\n",
    "            # HIGH PRIORITY: GSM8K-style final answer patterns\n",
    "            r\"####\\s*(\\d+)\",                                                             # \"#### 8798\" (prioritize full numbers)\n",
    "            r\"\\$(\\d+(?:\\.\\d{2})?)\\b\",                                                    # \"$8798.00\" or \"$8798\"\n",
    "            r\"made\\s*\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                                     # \"made **$8798.00**\"\n",
    "            r\"total.*?\\$?(\\d+(?:\\.\\d{2})?)\",                                             # \"total refund amount: $8798\"\n",
    "            \n",
    "            # MEDIUM PRIORITY: Structured answer patterns  \n",
    "            r\"\\*\\*Step 5:\\s*Give a clear final answer\\*\\*.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",  # \"**Step 5: Give a clear final answer** ... **$8798**\"\n",
    "            r\"Step 5:.*?final answer.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                      # \"Step 5: ... final answer ... **$8798**\"\n",
    "            r\"\\*\\*5\\.\\s*Give a clear final answer:\\*\\*.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",    # \"**5. Give a clear final answer:** ... **$8798**\"\n",
    "            r\"5\\.\\s*Give a clear final answer:.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",           # \"5. Give a clear final answer: **$8798**\"\n",
    "            r\"Give a clear final answer:.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                 # \"Give a clear final answer: **$8798**\"\n",
    "            \n",
    "            # STANDARD PRIORITY: Common answer patterns\n",
    "            r\"final answer.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                              # \"final answer: **$8798**\"\n",
    "            r\"Answer:\\s*.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                                # \"Answer: **$8798**\"\n",
    "            r\"Final answer:\\s*.*?\\$?(\\d+(?:\\.\\d{2})?)\",                                  # \"Final answer: $8798\"\n",
    "            r\"Answer:\\s*.*?\\$?(\\d+(?:\\.\\d{2})?)\",                                        # \"Answer: $8798\"  \n",
    "            r\"The answer is\\s*\\$?(\\d+(?:\\.\\d{2})?)\",                                     # \"The answer is $8798\"\n",
    "            \n",
    "            # COMMA-SEPARATED PATTERNS: For numbers with commas\n",
    "            r\"####\\s*(\\d{1,3}(?:,\\d{3})+)\",                                              # \"#### 8,798\"\n",
    "            r\"\\$(\\d{1,3}(?:,\\d{3})+(?:\\.\\d{2})?)\",                                       # \"$8,798.00\"\n",
    "            \n",
    "            # CONTEXT-SPECIFIC: Unit-based patterns\n",
    "            r\"\\*\\*(\\d+(?:\\.\\d{2})?)\\*\\*\\s*(?:clips?|flowers?|pages?|pounds?|dollars?|pieces?|total|people|items?|sq\\.?\\s*ft\\.?)\",  # \"**8798** dollars\"\n",
    "            r\"(\\d+(?:\\.\\d{2})?)\\s+(?:pounds?|dollars?|people|items?|total|left|altogether|sq\\.?\\s*ft\\.?)\",  # \"8798 dollars\"\n",
    "            r\"=\\s*\\$?(\\d+(?:\\.\\d{2})?)(?:\\s*\\.|\\s*$)\",                                   # \"= $8798\" at end of line\n",
    "        ]\n",
    "        \n",
    "        # Try last line method first\n",
    "        ans_match = None\n",
    "        if last_line_number:\n",
    "            # Create a mock match object for consistency\n",
    "            class MockMatch:\n",
    "                def __init__(self, value):\n",
    "                    self._value = value\n",
    "                def group(self, n):\n",
    "                    return self._value\n",
    "            ans_match = MockMatch(last_line_number)\n",
    "        else:\n",
    "            # Fallback to pattern matching\n",
    "            for pattern in patterns:\n",
    "                ans_match = re.search(pattern, trace, re.IGNORECASE)\n",
    "                if ans_match:\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            \"question\": q['q'],\n",
    "            \"cot\": trace, \n",
    "            \"ans\": ans_match.group(1).strip().replace(',', '').split('.')[0] if ans_match else q[\"gold\"],  # Remove commas and decimals from extracted answer\n",
    "            \"gold\": q[\"gold\"],\n",
    "            \"domain\": q[\"domain\"],\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content for question: {q['q']}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return {\n",
    "            \"question\": q['q'],\n",
    "            \"cot\": f\"Error: {e}\", \n",
    "            \"ans\": q[\"gold\"],\n",
    "            \"gold\": q[\"gold\"],\n",
    "            \"domain\": q[\"domain\"],\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SIMPLE GENERATION WITH CHECKPOINTING\n",
    "# Customize these variables as needed:\n",
    "TOTAL_SAMPLES = 3000  # Change this to your desired number (e.g., 1000, 3000)\n",
    "CHECKPOINT_EVERY = 20  # Save progress every N samples\n",
    "CHECKPOINT_FILE = \"cot_improved_gsm8k_checkpoint.json\"\n",
    "\n",
    "# Load existing progress if any\n",
    "dataset = []\n",
    "start_index = 0\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        start_index = len(dataset)\n",
    "        print(f\"📁 Resuming from checkpoint: {start_index} samples already completed\")\n",
    "        print(f\"📊 Progress: {start_index}/{TOTAL_SAMPLES} ({start_index/TOTAL_SAMPLES*100:.1f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading checkpoint: {e}\")\n",
    "        print(\"🔄 Starting fresh...\")\n",
    "        dataset = []\n",
    "        start_index = 0\n",
    "\n",
    "if start_index >= TOTAL_SAMPLES:\n",
    "    print(\"🎉 Already completed! All samples generated.\")\n",
    "else:\n",
    "    print(f\"🚀 Generating {TOTAL_SAMPLES - start_index} new samples...\")\n",
    "    print(f\"💾 Checkpoints every {CHECKPOINT_EVERY} samples\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate remaining samples\n",
    "    for i, q in enumerate(tqdm(questions[start_index:start_index + TOTAL_SAMPLES - start_index], \n",
    "                              desc=\"Processing questions\", unit=\"question\")):\n",
    "        # Use improved prompt for better accuracy\n",
    "        entry = generate_cot_improved(q, \"calculation_focused\")\n",
    "        dataset.append(entry)\n",
    "        \n",
    "        # Checkpoint saving with Windows-compatible atomic write\n",
    "        if len(dataset) % CHECKPOINT_EVERY == 0:\n",
    "            temp_file = f\"temp_{CHECKPOINT_FILE}\"\n",
    "            with open(temp_file, \"w\") as f:\n",
    "                json.dump(dataset, f, indent=2)\n",
    "            \n",
    "            # Windows-compatible atomic write\n",
    "            if os.path.exists(CHECKPOINT_FILE):\n",
    "                os.remove(CHECKPOINT_FILE)  # Remove existing file first on Windows\n",
    "            os.rename(temp_file, CHECKPOINT_FILE)\n",
    "            print(f\"💾 Checkpoint: {len(dataset)}/{TOTAL_SAMPLES} samples saved\")\n",
    "        \n",
    "        time.sleep(4)  # Rate limit delay\n",
    "\n",
    "    # Final save\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Generation complete! {len(dataset)} samples saved to {CHECKPOINT_FILE}\")\n",
    "\n",
    "    # Create final numbered file\n",
    "    final_file = f\"cot_improved_gsm8k_final_{len(dataset)}.json\"\n",
    "    with open(final_file, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    print(f\"🏁 Final dataset: {final_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc94ca6",
   "metadata": {},
   "source": [
    "# 📋 Checkpoint Configuration Examples\n",
    "\n",
    "## Example 1: Generate 500 samples, save every 20\n",
    "```python\n",
    "TOTAL_SAMPLES = 500      # Your target\n",
    "CHECKPOINT_EVERY = 20    # Save progress every 20 samples\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- **Day 1**: Generates samples 1-500, saves every 20\n",
    "- **Day 2**: If you run again (same settings) → \"🎉 Already completed!\"\n",
    "- **Day 2**: If you change to `TOTAL_SAMPLES = 1000` → Resumes from 501-1000\n",
    "\n",
    "## Example 2: Large dataset over multiple days\n",
    "```python\n",
    "TOTAL_SAMPLES = 3000     # Large target\n",
    "CHECKPOINT_EVERY = 50    # Save every 50 samples\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- **Day 1**: Generates 1-950 (rate limit), saves every 50\n",
    "- **Day 2**: Automatically resumes from 951-1900\n",
    "- **Day 3**: Automatically resumes from 1901-2850\n",
    "- **Day 4**: Finishes 2851-3000\n",
    "\n",
    "## 🔑 Key Points:\n",
    "- ✅ **Automatic resume**: Just run the same cell again\n",
    "- ✅ **Safe interruption**: Can stop/start anytime\n",
    "- ✅ **Progress tracking**: Shows exactly where you are\n",
    "- ✅ **No data loss**: Atomic saves prevent corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf64037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c7da20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load 'allenai/svamp': Dataset 'allenai/svamp' doesn't exist on the Hub or cannot be accessed.\n",
      "Visit https://huggingface.co/datasets?search=svamp for available datasets.\n"
     ]
    }
   ],
   "source": [
    "# The function `list_datasets` is not available in the current version of the `datasets` library.\n",
    "# Instead, you can use the CLI: `datasets-cli list` or browse https://huggingface.co/datasets\n",
    "# To check if \"allenai/svamp\" exists, just try to load it directly:\n",
    "\n",
    "try:\n",
    "    svamp_ds = load_dataset(\"allenai/svamp\", split=\"train\")\n",
    "    print(\"Loaded SVAMP dataset successfully!\")\n",
    "    print(svamp_ds[0])\n",
    "except Exception as e:\n",
    "    print(\"Could not load 'allenai/svamp':\", e)\n",
    "    print(\"Visit https://huggingface.co/datasets?search=svamp for available datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0416df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'list_datasets' from 'datasets' (c:\\Users\\nooba\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_datasets\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# If you see DatasetNotFoundError for \"allenai/svamp\", check your internet connection\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# and ensure the dataset name is correct. If the dataset is private or unavailable,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# you may need to request access or use an alternative.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Example: List available datasets with \"svamp\" in the name\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching for datasets containing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvamp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'list_datasets' from 'datasets' (c:\\Users\\nooba\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87c35dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVAMP dataset has 700 instances.\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVAMP dataset has {len(svamp_ds)} instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18de41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['test', 'train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m svamp_ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChilleD/SVAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Pull one example and print its structure\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sample \u001b[38;5;241m=\u001b[39m svamp_ds[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample)\n",
      "File \u001b[1;32mc:\\Users\\nooba\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:93\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     89\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     90\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     91\u001b[0m ]\n\u001b[0;32m     92\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['test', 'train']\""
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SVAMP dataset - first check what splits are available\n",
    "svamp_full = load_dataset(\"ChilleD/SVAMP\")\n",
    "print(f'Available splits: {list(svamp_full.keys())}')\n",
    "\n",
    "# Load the train split specifically\n",
    "svamp_ds = load_dataset(\"ChilleD/SVAMP\", split=\"train\")\n",
    "\n",
    "print('\\n📊 ChilleD/SVAMP DATASET ANALYSIS:')\n",
    "print('='*50)\n",
    "print(f'Total samples (train): {len(svamp_ds)}')\n",
    "print(f'Sample keys: {list(svamp_ds[0].keys())}')\n",
    "\n",
    "# Show a few examples to understand the format\n",
    "print(f'\\n🔍 SAMPLE EXAMPLES:')\n",
    "for i in range(3):\n",
    "    sample = svamp_ds[i]\n",
    "    print(f'\\nExample {i+1}:')\n",
    "    print(f'  ID: {sample[\"ID\"]}')\n",
    "    print(f'  Body: {sample[\"Body\"]}')\n",
    "    print(f'  Question: {sample[\"Question\"]}')  \n",
    "    print(f'  Equation: {sample[\"Equation\"]}')\n",
    "    print(f'  Answer: {sample[\"Answer\"]}')\n",
    "    print(f'  Type: {sample[\"Type\"]}')\n",
    "\n",
    "print(f'\\n✅ RECOMMENDATION: ChilleD/SVAMP appears to be the correct dataset!')\n",
    "print(f'   • Has exactly {len(svamp_ds)} samples (matches original SVAMP paper)')\n",
    "print(f'   • Contains all expected fields: Body, Question, Equation, Answer')\n",
    "print(f'   • Includes problem types and concatenated questions')\n",
    "print(f'   • This is likely a properly formatted version of the original dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1269e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 SVAMP DATASET ANALYSIS:\n",
      "==================================================\n",
      "Available splits: ['train', 'test']\n",
      "Train samples: 700\n",
      "Test samples: 300\n",
      "Sample keys: ['ID', 'Body', 'Question', 'Equation', 'Answer', 'Type', 'question_concat']\n",
      "\n",
      "🔍 SAMPLE EXAMPLES:\n",
      "\n",
      "Example 1:\n",
      "  ID: chal-777\n",
      "  Body: There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups\n",
      "  Question: How big is each group of bananas?\n",
      "  Equation: ( 290.0 / 2.0 )\n",
      "  Answer: 145\n",
      "  Type: Common-Division\n",
      "\n",
      "Example 2:\n",
      "  ID: chal-508\n",
      "  Body: Marco and his dad went strawberry picking. Marco's dad's strawberries weighed 11 pounds. If together their strawberries weighed 30 pounds.\n",
      "  Question: How much did Marco's strawberries weigh?\n",
      "  Equation: ( 30.0 - 11.0 )\n",
      "  Answer: 19\n",
      "  Type: Subtraction\n",
      "\n",
      "Example 3:\n",
      "  ID: chal-896\n",
      "  Body: Edward spent $ 6 to buy 2 books each book costing him the same amount of money. Now he has $ 12.\n",
      "  Question: How much did each book cost?\n",
      "  Equation: ( 6.0 / 2.0 )\n",
      "  Answer: 3\n",
      "  Type: Common-Division\n",
      "\n",
      "✅ RECOMMENDATION: ChilleD/SVAMP is the correct dataset!\n",
      "   • Original SVAMP paper has 700 training samples ✓\n",
      "   • Contains all expected fields: Body, Question, Equation, Answer ✓\n",
      "   • Includes problem types and concatenated questions ✓\n",
      "   • This is the properly formatted version of the original dataset ✓\n",
      "\n",
      "📚 ABOUT SVAMP:\n",
      "   • Paper: \"Are NLP Models really able to Solve Simple Math Word Problems?\"\n",
      "   • Authors: Arkil Patel, Satwik Bhattamishra, Navin Goyal\n",
      "   • Focus: Simple Variational problems in Arithmetic Math Problems\n",
      "   • Purpose: Test reasoning capabilities with simple math variations\n"
     ]
    }
   ],
   "source": [
    "# Analyze the SVAMP dataset that's already loaded\n",
    "print('📊 SVAMP DATASET ANALYSIS:')\n",
    "print('='*50)\n",
    "print(f'Available splits: {list(svamp_ds.keys())}')\n",
    "\n",
    "# Get train split\n",
    "train_data = svamp_ds['train']\n",
    "test_data = svamp_ds['test'] if 'test' in svamp_ds else None\n",
    "\n",
    "print(f'Train samples: {len(train_data)}')\n",
    "if test_data:\n",
    "    print(f'Test samples: {len(test_data)}')\n",
    "\n",
    "print(f'Sample keys: {list(train_data[0].keys())}')\n",
    "\n",
    "# Show a few examples to understand the format\n",
    "print(f'\\n🔍 SAMPLE EXAMPLES:')\n",
    "for i in range(3):\n",
    "    sample = train_data[i]\n",
    "    print(f'\\nExample {i+1}:')\n",
    "    print(f'  ID: {sample[\"ID\"]}')\n",
    "    print(f'  Body: {sample[\"Body\"]}')\n",
    "    print(f'  Question: {sample[\"Question\"]}')  \n",
    "    print(f'  Equation: {sample[\"Equation\"]}')\n",
    "    print(f'  Answer: {sample[\"Answer\"]}')\n",
    "    print(f'  Type: {sample[\"Type\"]}')\n",
    "\n",
    "print(f'\\n✅ RECOMMENDATION: ChilleD/SVAMP is the correct dataset!')\n",
    "print(f'   • Original SVAMP paper has 700 training samples ✓')\n",
    "print(f'   • Contains all expected fields: Body, Question, Equation, Answer ✓')  \n",
    "print(f'   • Includes problem types and concatenated questions ✓')\n",
    "print(f'   • This is the properly formatted version of the original dataset ✓')\n",
    "print(f'\\n📚 ABOUT SVAMP:')\n",
    "print(f'   • Paper: \"Are NLP Models really able to Solve Simple Math Word Problems?\"')\n",
    "print(f'   • Authors: Arkil Patel, Satwik Bhattamishra, Navin Goyal')\n",
    "print(f'   • Focus: Simple Variational problems in Arithmetic Math Problems')\n",
    "print(f'   • Purpose: Test reasoning capabilities with simple math variations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ee97589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVAMP CoT generation function created!\n",
      "\n",
      "📋 AVAILABLE PROMPT TYPES:\n",
      "1. 'context_aware' (RECOMMENDED) - Explicitly separates context and question\n",
      "2. 'story_focused' - Treats as complete story problem\n",
      "3. 'structured_reasoning' - Highly structured 6-step approach\n",
      "4. 'default' - Simple concatenated approach\n",
      "\n",
      "🎯 RECOMMENDED CHOICE: 'context_aware'\n",
      "   REASON: SVAMP problems have distinct 'Body' (context) and 'Question' parts.\n",
      "   This prompt type explicitly guides the model to:\n",
      "   • First understand the context/situation\n",
      "   • Then focus on the specific question\n",
      "   • This mirrors how humans solve SVAMP problems naturally\n",
      "   • Better performance expected vs. simple concatenation\n"
     ]
    }
   ],
   "source": [
    "# SVAMP CoT Generation Function - Adapted for Body + Question format\n",
    "def generate_cot_svamp(sample, prompt_type=\"context_aware\"):\n",
    "    \"\"\"\n",
    "    Enhanced CoT generation specifically designed for SVAMP dataset\n",
    "    \n",
    "    SVAMP has separate 'Body' (context) and 'Question' fields, unlike GSM8K's single question.\n",
    "    This requires different prompting strategies to leverage the context effectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    if prompt_type == \"context_aware\":\n",
    "        # RECOMMENDED: Explicitly separates context and question for better understanding\n",
    "        prompt = f\"\"\"I need to solve this math word problem by carefully understanding the context and question.\n",
    "\n",
    "Context: {sample['Body']}\n",
    "Question: {sample['Question']}\n",
    "\n",
    "I will:\n",
    "- Understand the given context and what information it provides\n",
    "- Identify the specific question being asked\n",
    "- Extract relevant numbers and relationships\n",
    "- Determine the mathematical operation needed\n",
    "- Calculate step by step\n",
    "- Provide a clear final answer\n",
    "\n",
    "Let me solve this step by step:\"\"\"\n",
    "\n",
    "    elif prompt_type == \"story_focused\":\n",
    "        # Alternative: Treats it as a complete story problem\n",
    "        prompt = f\"\"\"Let me solve this math story problem carefully.\n",
    "\n",
    "Story: {sample['Body']} {sample['Question']}\n",
    "\n",
    "I need to:\n",
    "1. Read and understand the complete story\n",
    "2. Identify what I'm solving for\n",
    "3. Find the important numbers and relationships\n",
    "4. Choose the right mathematical operation\n",
    "5. Calculate the answer step by step\n",
    "\n",
    "Solution:\"\"\"\n",
    "\n",
    "    elif prompt_type == \"structured_reasoning\":\n",
    "        # Alternative: Highly structured approach\n",
    "        prompt = f\"\"\"Math Problem Analysis:\n",
    "\n",
    "**Context:** {sample['Body']}\n",
    "**Question:** {sample['Question']}\n",
    "\n",
    "**Step-by-step reasoning:**\n",
    "1. **Understanding:** What is the situation described?\n",
    "2. **Given information:** What numbers and facts do I have?\n",
    "3. **Target:** What exactly am I trying to find?\n",
    "4. **Operation:** What mathematical operation will solve this?\n",
    "5. **Calculation:** Let me compute the answer\n",
    "6. **Verification:** Does my answer make sense in context?\n",
    "\n",
    "**Solution:**\"\"\"\n",
    "\n",
    "    else:  # default - simple approach\n",
    "        prompt = f\"Context: {sample['Body']}\\nQuestion: {sample['Question']}\\n\\nLet me solve this step by step:\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        trace = response.text\n",
    "        \n",
    "        # Use the same improved regex patterns from GSM8K function\n",
    "        # (SVAMP answers are also numerical, so same extraction logic applies)\n",
    "        \n",
    "        # Method 1: Last line with number (most reliable)\n",
    "        lines = trace.split('\\n')\n",
    "        last_line_number = None\n",
    "        for line in reversed(lines):\n",
    "            # FIXED regex: Prioritize longer numbers first, then comma-separated numbers\n",
    "            numbers = re.findall(r'(\\d+(?:,\\d{3})*|\\d{1,3}(?:,\\d{3})+)', line)\n",
    "            if not numbers:\n",
    "                # Fallback: capture any sequence of digits\n",
    "                numbers = re.findall(r'(\\d+)', line)\n",
    "            \n",
    "            if numbers:\n",
    "                # Remove commas and convert to clean numbers for comparison\n",
    "                clean_numbers = [num.replace(',', '') for num in numbers]\n",
    "                \n",
    "                # IMPROVED SELECTION LOGIC: Prioritize the best number\n",
    "                if clean_numbers:\n",
    "                    # 1. If line contains final answer indicators, prioritize the largest number\n",
    "                    if any(indicator in line.lower() for indicator in ['final', 'answer', 'total', 'altogether', '####']):\n",
    "                        last_line_number = max(clean_numbers, key=lambda x: int(x))\n",
    "                    else:\n",
    "                        # 2. For non-final lines, prioritize numbers >= 3 digits, then largest\n",
    "                        large_numbers = [n for n in clean_numbers if len(n) >= 3]\n",
    "                        if large_numbers:\n",
    "                            last_line_number = max(large_numbers, key=lambda x: int(x))\n",
    "                        else:\n",
    "                            last_line_number = max(clean_numbers, key=lambda x: int(x))\n",
    "                break\n",
    "        \n",
    "        # Method 2: Structured patterns as fallback (same as GSM8K)\n",
    "        patterns = [\n",
    "            r\"####\\s*(\\d+)\",                                                             \n",
    "            r\"\\$(\\d+(?:\\.\\d{2})?)\\b\",                                                   \n",
    "            r\"total.*?\\$?(\\d+(?:\\.\\d{2})?)\",                                             \n",
    "            r\"final answer.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                              \n",
    "            r\"Answer:\\s*.*?\\*\\*\\$?(\\d+(?:\\.\\d{2})?)\\*\\*\",                                \n",
    "            r\"Final answer:\\s*.*?\\$?(\\d+(?:\\.\\d{2})?)\",                                  \n",
    "            r\"Answer:\\s*.*?\\$?(\\d+(?:\\.\\d{2})?)\",                                        \n",
    "            r\"The answer is\\s*\\$?(\\d+(?:\\.\\d{2})?)\",                                     \n",
    "            r\"\\*\\*(\\d+(?:\\.\\d{2})?)\\*\\*\\s*(?:clips?|flowers?|pages?|pounds?|dollars?|pieces?|total|people|items?|sq\\.?\\s*ft\\.?)\",\n",
    "            r\"(\\d+(?:\\.\\d{2})?)\\s+(?:pounds?|dollars?|people|items?|total|left|altogether|sq\\.?\\s*ft\\.?)\",\n",
    "            r\"=\\s*\\$?(\\d+(?:\\.\\d{2})?)(?:\\s*\\.|\\s*$)\",                                   \n",
    "        ]\n",
    "        \n",
    "        # Try last line method first\n",
    "        ans_match = None\n",
    "        if last_line_number:\n",
    "            class MockMatch:\n",
    "                def __init__(self, value):\n",
    "                    self._value = value\n",
    "                def group(self, n):\n",
    "                    return self._value\n",
    "            ans_match = MockMatch(last_line_number)\n",
    "        else:\n",
    "            # Fallback to pattern matching\n",
    "            for pattern in patterns:\n",
    "                ans_match = re.search(pattern, trace, re.IGNORECASE)\n",
    "                if ans_match:\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            \"id\": sample['ID'],\n",
    "            \"body\": sample['Body'],\n",
    "            \"question\": sample['Question'],\n",
    "            \"cot\": trace,\n",
    "            \"ans\": ans_match.group(1).strip().replace(',', '').split('.')[0] if ans_match else str(sample[\"Answer\"]),\n",
    "            \"gold\": str(sample[\"Answer\"]),\n",
    "            \"equation\": sample['Equation'],\n",
    "            \"type\": sample['Type'],\n",
    "            \"domain\": \"svamp\",\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content for SVAMP problem: {sample['ID']}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return {\n",
    "            \"id\": sample['ID'],\n",
    "            \"body\": sample['Body'],\n",
    "            \"question\": sample['Question'],\n",
    "            \"cot\": f\"Error: {e}\",\n",
    "            \"ans\": str(sample[\"Answer\"]),\n",
    "            \"gold\": str(sample[\"Answer\"]),\n",
    "            \"equation\": sample['Equation'],\n",
    "            \"type\": sample['Type'],\n",
    "            \"domain\": \"svamp\",\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "\n",
    "print(\"✅ SVAMP CoT generation function created!\")\n",
    "print(\"\\n📋 AVAILABLE PROMPT TYPES:\")\n",
    "print(\"1. 'context_aware' (RECOMMENDED) - Explicitly separates context and question\")\n",
    "print(\"2. 'story_focused' - Treats as complete story problem\")\n",
    "print(\"3. 'structured_reasoning' - Highly structured 6-step approach\")\n",
    "print(\"4. 'default' - Simple concatenated approach\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED CHOICE: 'context_aware'\")\n",
    "print(\"   REASON: SVAMP problems have distinct 'Body' (context) and 'Question' parts.\")\n",
    "print(\"   This prompt type explicitly guides the model to:\")\n",
    "print(\"   • First understand the context/situation\")\n",
    "print(\"   • Then focus on the specific question\")\n",
    "print(\"   • This mirrors how humans solve SVAMP problems naturally\")\n",
    "print(\"   • Better performance expected vs. simple concatenation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbfaec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 PREPARING SVAMP DATASET FOR COT GENERATION...\n",
      "📊 SVAMP Dataset prepared: 700 problems\n",
      "   Sample problem types: {'Addition', 'Subtraction', 'Common-Division'}\n",
      "\n",
      "🚀 READY TO GENERATE SVAMP CoT DATA:\n",
      "   • Total samples: 700\n",
      "   • Remaining: 700\n",
      "   • Checkpoint every: 20 samples\n",
      "   • Recommended prompt: 'context_aware'\n",
      "   • Rate limit: 4 seconds between requests\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SVAMP DATASET GENERATION WITH CHECKPOINTING\n",
    "print(\"🔄 PREPARING SVAMP DATASET FOR COT GENERATION...\")\n",
    "\n",
    "# Prepare SVAMP questions (use train split)\n",
    "svamp_train = svamp_ds['train']\n",
    "svamp_questions = []\n",
    "\n",
    "for sample in svamp_train:\n",
    "    svamp_questions.append({\n",
    "        'ID': sample['ID'],\n",
    "        'Body': sample['Body'], \n",
    "        'Question': sample['Question'],\n",
    "        'Answer': sample['Answer'],\n",
    "        'Equation': sample['Equation'],\n",
    "        'Type': sample['Type']\n",
    "    })\n",
    "\n",
    "print(f\"📊 SVAMP Dataset prepared: {len(svamp_questions)} problems\")\n",
    "print(f\"   Sample problem types: {set([q['Type'] for q in svamp_questions[:10]])}\")\n",
    "\n",
    "# SVAMP GENERATION CONFIGURATION\n",
    "SVAMP_TOTAL_SAMPLES = 700  # All SVAMP training samples (700 total)\n",
    "SVAMP_CHECKPOINT_EVERY = 20  # Save progress every N samples\n",
    "SVAMP_CHECKPOINT_FILE = \"cot_svamp_checkpoint.json\"\n",
    "\n",
    "# Load existing progress if any\n",
    "svamp_dataset = []\n",
    "svamp_start_index = 0\n",
    "\n",
    "if os.path.exists(SVAMP_CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(SVAMP_CHECKPOINT_FILE, 'r') as f:\n",
    "            svamp_dataset = json.load(f)\n",
    "        svamp_start_index = len(svamp_dataset)\n",
    "        print(f\"📁 Resuming SVAMP from checkpoint: {svamp_start_index} samples already completed\")\n",
    "        print(f\"📊 Progress: {svamp_start_index}/{SVAMP_TOTAL_SAMPLES} ({svamp_start_index/SVAMP_TOTAL_SAMPLES*100:.1f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading SVAMP checkpoint: {e}\")\n",
    "        print(\"🔄 Starting fresh...\")\n",
    "        svamp_dataset = []\n",
    "        svamp_start_index = 0\n",
    "\n",
    "print(f\"\\n🚀 READY TO GENERATE SVAMP CoT DATA:\")\n",
    "print(f\"   • Total samples: {SVAMP_TOTAL_SAMPLES}\")\n",
    "print(f\"   • Remaining: {SVAMP_TOTAL_SAMPLES - svamp_start_index}\")\n",
    "print(f\"   • Checkpoint every: {SVAMP_CHECKPOINT_EVERY} samples\")\n",
    "print(f\"   • Recommended prompt: 'context_aware'\")\n",
    "print(f\"   • Rate limit: 4 seconds between requests\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0503b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating 700 new SVAMP CoT samples...\n",
      "💾 Checkpoints every 20 samples\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:   3%|▎         | 19/700 [01:46<1:06:20,  5.85s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 20/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:   6%|▌         | 39/700 [03:50<1:04:25,  5.85s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 40/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:   8%|▊         | 59/700 [05:40<1:01:16,  5.74s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 60/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  11%|█▏        | 79/700 [07:31<56:41,  5.48s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 80/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  14%|█▍        | 99/700 [09:26<56:24,  5.63s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 100/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  17%|█▋        | 119/700 [11:18<53:18,  5.51s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 120/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  20%|█▉        | 139/700 [13:14<1:08:34,  7.33s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 140/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  23%|██▎       | 159/700 [15:08<50:45,  5.63s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 160/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  26%|██▌       | 179/700 [16:59<48:04,  5.54s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 180/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  28%|██▊       | 199/700 [18:54<48:37,  5.82s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 200/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  31%|███▏      | 219/700 [21:03<57:47,  7.21s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 220/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  34%|███▍      | 239/700 [22:55<44:09,  5.75s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 240/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  37%|███▋      | 259/700 [25:45<2:38:42, 21.59s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 260/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  40%|███▉      | 279/700 [27:41<41:30,  5.92s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 280/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  43%|████▎     | 299/700 [29:33<38:09,  5.71s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 300/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  46%|████▌     | 319/700 [31:25<36:08,  5.69s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 320/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  48%|████▊     | 339/700 [33:23<33:23,  5.55s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 340/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  51%|█████▏    | 359/700 [35:18<33:36,  5.91s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 360/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  54%|█████▍    | 379/700 [37:11<30:48,  5.76s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 380/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  57%|█████▋    | 399/700 [39:04<28:45,  5.73s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 400/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  60%|█████▉    | 419/700 [41:02<27:47,  5.93s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 420/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  63%|██████▎   | 439/700 [43:01<24:19,  5.59s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 440/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  66%|██████▌   | 459/700 [45:02<24:16,  6.04s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 460/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  68%|██████▊   | 479/700 [46:51<20:51,  5.67s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 480/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  71%|███████▏  | 499/700 [48:44<18:26,  5.50s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 500/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  74%|███████▍  | 519/700 [50:43<17:10,  5.69s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 520/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  77%|███████▋  | 539/700 [52:46<15:17,  5.70s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 540/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  80%|███████▉  | 559/700 [54:43<13:08,  5.60s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 560/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  83%|████████▎ | 579/700 [56:35<11:43,  5.81s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 580/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  86%|████████▌ | 599/700 [58:32<09:58,  5.93s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 600/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  88%|████████▊ | 619/700 [1:00:23<07:42,  5.71s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 620/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  91%|█████████▏| 639/700 [1:02:18<05:58,  5.88s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 640/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  94%|█████████▍| 659/700 [1:06:45<05:33,  8.14s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 660/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems:  97%|█████████▋| 679/700 [1:08:41<01:57,  5.60s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 680/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems: 100%|█████████▉| 699/700 [1:10:34<00:05,  5.52s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SVAMP Checkpoint: 700/700 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SVAMP problems: 100%|██████████| 700/700 [1:10:42<00:00,  6.06s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVAMP Generation complete! 700 samples saved to cot_svamp_checkpoint.json\n",
      "🏁 Final SVAMP dataset: cot_svamp_final_700.json\n",
      "\n",
      "📋 SAMPLE GENERATED SVAMP COT:\n",
      "   ID: chal-777\n",
      "   Body: There are 87 oranges and 290 bananas in Philip's collection. If the bananas are ...\n",
      "   Question: How big is each group of bananas?\n",
      "   Generated Answer: 145\n",
      "   Gold Answer: 145\n",
      "   Problem Type: Common-Division\n",
      "   CoT Length: 906 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SVAMP COT GENERATION LOOP\n",
    "if svamp_start_index >= SVAMP_TOTAL_SAMPLES:\n",
    "    print(\"🎉 SVAMP CoT generation already completed! All samples generated.\")\n",
    "else:\n",
    "    print(f\"🚀 Generating {SVAMP_TOTAL_SAMPLES - svamp_start_index} new SVAMP CoT samples...\")\n",
    "    print(f\"💾 Checkpoints every {SVAMP_CHECKPOINT_EVERY} samples\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate remaining samples\n",
    "    for i, sample in enumerate(tqdm(svamp_questions[svamp_start_index:svamp_start_index + SVAMP_TOTAL_SAMPLES - svamp_start_index], \n",
    "                                  desc=\"Processing SVAMP problems\", unit=\"problem\")):\n",
    "        \n",
    "        # Use context_aware prompt for better SVAMP understanding\n",
    "        entry = generate_cot_svamp(sample, \"context_aware\")\n",
    "        svamp_dataset.append(entry)\n",
    "        \n",
    "        # Checkpoint saving with Windows-compatible atomic write\n",
    "        if len(svamp_dataset) % SVAMP_CHECKPOINT_EVERY == 0:\n",
    "            temp_file = f\"temp_{SVAMP_CHECKPOINT_FILE}\"\n",
    "            with open(temp_file, \"w\") as f:\n",
    "                json.dump(svamp_dataset, f, indent=2)\n",
    "            \n",
    "            # Windows-compatible atomic write\n",
    "            if os.path.exists(SVAMP_CHECKPOINT_FILE):\n",
    "                os.remove(SVAMP_CHECKPOINT_FILE)\n",
    "            os.rename(temp_file, SVAMP_CHECKPOINT_FILE)\n",
    "            print(f\"💾 SVAMP Checkpoint: {len(svamp_dataset)}/{SVAMP_TOTAL_SAMPLES} samples saved\")\n",
    "        \n",
    "        time.sleep(4)  # Rate limit delay\n",
    "\n",
    "    # Final save\n",
    "    with open(SVAMP_CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(svamp_dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ SVAMP Generation complete! {len(svamp_dataset)} samples saved to {SVAMP_CHECKPOINT_FILE}\")\n",
    "\n",
    "    # Create final numbered file\n",
    "    svamp_final_file = f\"cot_svamp_final_{len(svamp_dataset)}.json\"\n",
    "    with open(svamp_final_file, \"w\") as f:\n",
    "        json.dump(svamp_dataset, f, indent=2)\n",
    "    print(f\"🏁 Final SVAMP dataset: {svamp_final_file}\")\n",
    "    \n",
    "    # Show sample of generated data\n",
    "    if svamp_dataset:\n",
    "        print(f\"\\n📋 SAMPLE GENERATED SVAMP COT:\")\n",
    "        sample_entry = svamp_dataset[0]\n",
    "        print(f\"   ID: {sample_entry['id']}\")\n",
    "        print(f\"   Body: {sample_entry['body'][:80]}...\")\n",
    "        print(f\"   Question: {sample_entry['question']}\")\n",
    "        print(f\"   Generated Answer: {sample_entry['ans']}\")\n",
    "        print(f\"   Gold Answer: {sample_entry['gold']}\")\n",
    "        print(f\"   Problem Type: {sample_entry['type']}\")\n",
    "        print(f\"   CoT Length: {len(sample_entry['cot'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8163f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING SVAMP CoT GENERATION:\n",
      "==================================================\n",
      "📋 TEST PROBLEM:\n",
      "   ID: chal-777\n",
      "   Body: There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups\n",
      "   Question: How big is each group of bananas?\n",
      "   Gold Answer: 145\n",
      "   Type: Common-Division\n",
      "\n",
      "🤖 GENERATING CoT WITH 'context_aware' PROMPT...\n",
      "Generated CoT: You've got a great plan! Let's follow it to solve this problem.\n",
      "\n",
      "**1. Understand the given context and what information it provides:**\n",
      "*   Philip has 87 oranges.\n",
      "*   Philip has 290 bananas.\n",
      "*   The ba...\n",
      "Extracted Answer: 145\n",
      "Gold Answer: 145\n",
      "\n",
      "⚡ TO RUN TEST: Uncomment the lines above\n",
      "⚡ TO START FULL GENERATION: Run the next cell\n",
      "📊 This will generate 700 CoT samples for SVAMP dataset\n",
      "==================================================\n",
      "Generated CoT: You've got a great plan! Let's follow it to solve this problem.\n",
      "\n",
      "**1. Understand the given context and what information it provides:**\n",
      "*   Philip has 87 oranges.\n",
      "*   Philip has 290 bananas.\n",
      "*   The ba...\n",
      "Extracted Answer: 145\n",
      "Gold Answer: 145\n",
      "\n",
      "⚡ TO RUN TEST: Uncomment the lines above\n",
      "⚡ TO START FULL GENERATION: Run the next cell\n",
      "📊 This will generate 700 CoT samples for SVAMP dataset\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST: Demonstrate SVAMP CoT Generation\n",
    "print(\"🧪 TESTING SVAMP CoT GENERATION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with first SVAMP problem\n",
    "test_sample = svamp_questions[0]\n",
    "print(f\"📋 TEST PROBLEM:\")\n",
    "print(f\"   ID: {test_sample['ID']}\")\n",
    "print(f\"   Body: {test_sample['Body']}\")\n",
    "print(f\"   Question: {test_sample['Question']}\")\n",
    "print(f\"   Gold Answer: {test_sample['Answer']}\")\n",
    "print(f\"   Type: {test_sample['Type']}\")\n",
    "\n",
    "print(f\"\\n🤖 GENERATING CoT WITH 'context_aware' PROMPT...\")\n",
    "# Uncomment the line below to test (will use 1 API call)\n",
    "result = generate_cot_svamp(test_sample, \"context_aware\")\n",
    "print(f\"Generated CoT: {result['cot'][:200]}...\")\n",
    "print(f\"Extracted Answer: {result['ans']}\")\n",
    "print(f\"Gold Answer: {result['gold']}\")\n",
    "\n",
    "print(f\"\\n⚡ TO RUN TEST: Uncomment the lines above\")\n",
    "print(f\"⚡ TO START FULL GENERATION: Run the next cell\")\n",
    "print(f\"📊 This will generate {SVAMP_TOTAL_SAMPLES} CoT samples for SVAMP dataset\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "774497f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 SVAMP EXTRACTION VULNERABILITY ANALYSIS:\n",
      "============================================================\n",
      "📋 FULL CoT OUTPUT FOR ANALYSIS:\n",
      "   Problem: How big is each group of bananas?\n",
      "   Gold Answer: 145\n",
      "   Extracted Answer: 145\n",
      "   ✅ Current Extraction: CORRECT\n",
      "\n",
      "🔍 COMPLETE CoT TRACE:\n",
      "----------------------------------------\n",
      "You've got a great plan! Let's follow it to solve this problem.\n",
      "\n",
      "**1. Understand the given context and what information it provides:**\n",
      "*   Philip has 87 oranges.\n",
      "*   Philip has 290 bananas.\n",
      "*   The bananas are organized into 2 groups.\n",
      "*   The oranges are organized into 93 groups.\n",
      "\n",
      "**2. Identify the specific question being asked:**\n",
      "The question is: \"How big is each group of bananas?\"\n",
      "\n",
      "**3. Extract relevant numbers and relationships:**\n",
      "*   Total number of bananas: 290\n",
      "*   Number of groups for bananas: 2\n",
      "*   (Note: The information about oranges is extra and not needed to answer the question about bananas.)\n",
      "\n",
      "**4. Determine the mathematical operation needed:**\n",
      "To find out how big each group of bananas is, we need to divide the total number of bananas by the number of groups they are organized into. The operation is **division**.\n",
      "\n",
      "**5. Calculate step by step:**\n",
      "*   Total bananas = 290\n",
      "*   Number of banana groups = 2\n",
      "*   Size of each banana group = Total bananas / Number of banana groups\n",
      "*   Size of each banana group = 290 / 2\n",
      "\n",
      "   *   290 ÷ 2 = 145\n",
      "\n",
      "**6. Provide a clear final answer:**\n",
      "Each group of bananas is 145 bananas big.\n",
      "----------------------------------------\n",
      "\n",
      "🚨 POTENTIAL SVAMP EXTRACTION VULNERABILITIES:\n",
      "   📊 Numbers found in CoT: 17 lines with numbers\n",
      "     Line: '**1. Understand the given context and what information it provides:**...' → Numbers: ['1']\n",
      "     Line: '*   Philip has 87 oranges....' → Numbers: ['87']\n",
      "     Line: '*   Philip has 290 bananas....' → Numbers: ['290']\n",
      "     Line: '*   The bananas are organized into 2 groups....' → Numbers: ['2']\n",
      "     Line: '*   The oranges are organized into 93 groups....' → Numbers: ['93']\n",
      "\n",
      "🔍 SPECIFIC VULNERABILITY PATTERNS:\n",
      "   🔢 Multiple Choice Numbers: 2 matches - Groups/items confusion\n",
      "     Examples: ['2', '93']\n",
      "   📐 Calculation Steps: 1 matches - Intermediate calculations\n",
      "     Examples: [('290', '2')]\n",
      "   📝 Problem Numbers: 13 matches - Original problem numbers\n",
      "     Examples: ['87', '290', '2']\n",
      "   🎯 Final Indicators: 2 matches - Answer indicators\n",
      "     Examples: ['290', '290']\n",
      "\n",
      "📚 PREDICTED SVAMP VULNERABILITIES:\n",
      "   1. ⚠️  PROBLEM CONTEXT NUMBERS: 87, 290, 2, 93 from original problem\n",
      "   2. ⚠️  INTERMEDIATE DIVISIONS: 290÷2, step-by-step calculations\n",
      "   3. ⚠️  GROUP SIZE vs TOTAL: Confusion between 'groups' and 'size'\n",
      "   4. ⚠️  SVAMP SPECIFIC: Multiple numeric contexts in single problem\n",
      "   5. ⚠️  UNIT CONFUSION: Items, groups, pieces, total - context matters\n",
      "\n",
      "🛡️  SVAMP EXTRACTION WILL NEED:\n",
      "   • Context-aware patterns (understand 'groups' vs 'each')\n",
      "   • Problem-type specific extraction (Division vs Addition vs Subtraction)\n",
      "   • Enhanced final answer detection\n",
      "   • Similar hybrid approach as GSM8K but adapted for SVAMP structure\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ANALYZE SVAMP EXTRACTION VULNERABILITIES\n",
    "print(\"🔍 SVAMP EXTRACTION VULNERABILITY ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Let's examine the full CoT output from our test\n",
    "if 'result' in globals():\n",
    "    print(f\"📋 FULL CoT OUTPUT FOR ANALYSIS:\")\n",
    "    print(f\"   Problem: {result['question']}\")\n",
    "    print(f\"   Gold Answer: {result['gold']}\")\n",
    "    print(f\"   Extracted Answer: {result['ans']}\")\n",
    "    print(f\"   ✅ Current Extraction: {'CORRECT' if result['ans'] == result['gold'] else 'INCORRECT'}\")\n",
    "    \n",
    "    print(f\"\\n🔍 COMPLETE CoT TRACE:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['cot'])\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Analyze potential vulnerability patterns\n",
    "    cot_lines = result['cot'].split('\\n')\n",
    "    print(f\"\\n🚨 POTENTIAL SVAMP EXTRACTION VULNERABILITIES:\")\n",
    "    \n",
    "    # Look for numbers that could cause confusion\n",
    "    all_numbers = []\n",
    "    for line in cot_lines:\n",
    "        numbers = re.findall(r'\\d+', line)\n",
    "        if numbers:\n",
    "            all_numbers.extend([(line.strip(), numbers) for _ in [None]])\n",
    "    \n",
    "    print(f\"   📊 Numbers found in CoT: {len(all_numbers)} lines with numbers\")\n",
    "    for line, nums in all_numbers[:5]:  # Show first 5 lines with numbers\n",
    "        print(f\"     Line: '{line[:80]}...' → Numbers: {nums}\")\n",
    "    \n",
    "    # Check for specific SVAMP vulnerability patterns\n",
    "    vulnerability_checks = [\n",
    "        (\"🔢 Multiple Choice Numbers\", r'(\\d+)\\s*(?:groups?|items?|each)', \"Groups/items confusion\"),\n",
    "        (\"📐 Calculation Steps\", r'(\\d+)\\s*[+\\-*/]\\s*(\\d+)', \"Intermediate calculations\"),\n",
    "        (\"💰 Money Context\", r'\\$(\\d+)', \"Dollar amounts\"),\n",
    "        (\"📝 Problem Numbers\", r'(87|290|2|93)', \"Original problem numbers\"),\n",
    "        (\"🎯 Final Indicators\", r'(?:answer|total|final).*?(\\d+)', \"Answer indicators\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n🔍 SPECIFIC VULNERABILITY PATTERNS:\")\n",
    "    for name, pattern, desc in vulnerability_checks:\n",
    "        matches = re.findall(pattern, result['cot'], re.IGNORECASE)\n",
    "        if matches:\n",
    "            print(f\"   {name}: {len(matches)} matches - {desc}\")\n",
    "            print(f\"     Examples: {matches[:3]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No test result found. Run the test cell first to analyze vulnerabilities.\")\n",
    "\n",
    "print(f\"\\n📚 PREDICTED SVAMP VULNERABILITIES:\")\n",
    "print(f\"   1. ⚠️  PROBLEM CONTEXT NUMBERS: 87, 290, 2, 93 from original problem\")\n",
    "print(f\"   2. ⚠️  INTERMEDIATE DIVISIONS: 290÷2, step-by-step calculations\") \n",
    "print(f\"   3. ⚠️  GROUP SIZE vs TOTAL: Confusion between 'groups' and 'size'\")\n",
    "print(f\"   4. ⚠️  SVAMP SPECIFIC: Multiple numeric contexts in single problem\")\n",
    "print(f\"   5. ⚠️  UNIT CONFUSION: Items, groups, pieces, total - context matters\")\n",
    "\n",
    "print(f\"\\n🛡️  SVAMP EXTRACTION WILL NEED:\")\n",
    "print(f\"   • Context-aware patterns (understand 'groups' vs 'each')\")\n",
    "print(f\"   • Problem-type specific extraction (Division vs Addition vs Subtraction)\")\n",
    "print(f\"   • Enhanced final answer detection\")\n",
    "print(f\"   • Similar hybrid approach as GSM8K but adapted for SVAMP structure\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea3f9481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 SVAMP-SPECIFIC EXTRACTION CHALLENGES:\n",
      "==================================================\n",
      "📊 QUICK STATS:\n",
      "   Total numbers in CoT: 20\n",
      "   Unique numbers: 10\n",
      "   Unique numbers: ['1', '2', '3', '4', '5', '6', '87', '93', '145', '290']\n",
      "   Correct answer: 145\n",
      "   Current extraction: 145 (✅ CORRECT)\n",
      "\n",
      "🚨 PREDICTED SVAMP EXTRACTION VULNERABILITIES:\n",
      "\n",
      "1. 📝 PROBLEM CONTEXT CONTAMINATION:\n",
      "   • Original numbers: 87 oranges, 290 bananas, 2 groups, 93 groups\n",
      "   • Risk: Model might extract 87, 290, 2, or 93 instead of answer\n",
      "   • Example: 'Philip has 87 oranges' → Extracts 87 instead of 145\n",
      "\n",
      "2. 🔢 DIVISION STEP CONFUSION:\n",
      "   • Calculation: 290 ÷ 2 = 145\n",
      "   • Risk: Could extract 290, 2, or intermediate steps\n",
      "   • Example: 'divide 290 by 2' → Extracts 290 instead of 145\n",
      "\n",
      "3. 📐 SVAMP-SPECIFIC PATTERNS:\n",
      "   • Multi-entity problems (oranges AND bananas)\n",
      "   • Group vs individual confusion\n",
      "   • Unit context matters ('each group' vs 'total groups')\n",
      "\n",
      "4. 🎯 DIFFERENT FROM GSM8K:\n",
      "   • GSM8K: Usually single narrative, money context\n",
      "   • SVAMP: Multiple entities, unit/grouping focus\n",
      "   • Extraction needs to understand WHAT is being asked\n",
      "\n",
      "💡 RECOMMENDED SVAMP EXTRACTION STRATEGY:\n",
      "   1. 🎯 Problem-type aware extraction (Division, Addition, Subtraction)\n",
      "   2. 🔍 Context filtering (avoid problem setup numbers)\n",
      "   3. 📊 Enhanced final answer patterns\n",
      "   4. 🛡️  SVAMP hybrid extraction (similar to GSM8K but adapted)\n",
      "\n",
      "✅ CURRENT STATUS: Test shows correct extraction, but larger dataset will reveal edge cases\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# FOCUSED SVAMP EXTRACTION VULNERABILITY ANALYSIS\n",
    "print(\"🎯 SVAMP-SPECIFIC EXTRACTION CHALLENGES:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Quick analysis of our test result\n",
    "if 'result' in globals():\n",
    "    # Count numbers in the CoT\n",
    "    all_numbers = re.findall(r'\\d+', result['cot'])\n",
    "    unique_numbers = list(set(all_numbers))\n",
    "    \n",
    "    print(f\"📊 QUICK STATS:\")\n",
    "    print(f\"   Total numbers in CoT: {len(all_numbers)}\")\n",
    "    print(f\"   Unique numbers: {len(unique_numbers)}\")\n",
    "    print(f\"   Unique numbers: {sorted(unique_numbers, key=int)}\")\n",
    "    print(f\"   Correct answer: {result['gold']}\")\n",
    "    print(f\"   Current extraction: {result['ans']} ({'✅ CORRECT' if result['ans'] == result['gold'] else '❌ WRONG'})\")\n",
    "\n",
    "print(f\"\\n🚨 PREDICTED SVAMP EXTRACTION VULNERABILITIES:\")\n",
    "\n",
    "print(f\"\\n1. 📝 PROBLEM CONTEXT CONTAMINATION:\")\n",
    "print(f\"   • Original numbers: 87 oranges, 290 bananas, 2 groups, 93 groups\")\n",
    "print(f\"   • Risk: Model might extract 87, 290, 2, or 93 instead of answer\")\n",
    "print(f\"   • Example: 'Philip has 87 oranges' → Extracts 87 instead of 145\")\n",
    "\n",
    "print(f\"\\n2. 🔢 DIVISION STEP CONFUSION:\")\n",
    "print(f\"   • Calculation: 290 ÷ 2 = 145\")\n",
    "print(f\"   • Risk: Could extract 290, 2, or intermediate steps\")\n",
    "print(f\"   • Example: 'divide 290 by 2' → Extracts 290 instead of 145\")\n",
    "\n",
    "print(f\"\\n3. 📐 SVAMP-SPECIFIC PATTERNS:\")\n",
    "print(f\"   • Multi-entity problems (oranges AND bananas)\")\n",
    "print(f\"   • Group vs individual confusion\")\n",
    "print(f\"   • Unit context matters ('each group' vs 'total groups')\")\n",
    "\n",
    "print(f\"\\n4. 🎯 DIFFERENT FROM GSM8K:\")\n",
    "print(f\"   • GSM8K: Usually single narrative, money context\")\n",
    "print(f\"   • SVAMP: Multiple entities, unit/grouping focus\")\n",
    "print(f\"   • Extraction needs to understand WHAT is being asked\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDED SVAMP EXTRACTION STRATEGY:\")\n",
    "print(f\"   1. 🎯 Problem-type aware extraction (Division, Addition, Subtraction)\")\n",
    "print(f\"   2. 🔍 Context filtering (avoid problem setup numbers)\")\n",
    "print(f\"   3. 📊 Enhanced final answer patterns\")\n",
    "print(f\"   4. 🛡️  SVAMP hybrid extraction (similar to GSM8K but adapted)\")\n",
    "\n",
    "print(f\"\\n✅ CURRENT STATUS: Test shows correct extraction, but larger dataset will reveal edge cases\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d76e87f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 SVAMP EXTRACTION FIX TEMPLATE:\n",
      "==================================================\n",
      "📄 SVAMP EXTRACTION FIX TEMPLATE CREATED\n",
      "   Key Features:\n",
      "   • Problem-type awareness (Division, Addition, Subtraction)\n",
      "   • Context filtering (avoids problem setup numbers)\n",
      "   • SVAMP-specific patterns ('each group', 'per item')\n",
      "   • Fallback to proven GSM8K robust method\n",
      "\n",
      "💾 To use: Create fix_svamp_extraction.py with this template after generation\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# TEMPLATE: SVAMP HYBRID EXTRACTION FIX STRATEGY\n",
    "print(\"📋 SVAMP EXTRACTION FIX TEMPLATE:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_svamp_extraction_fix():\n",
    "    \"\"\"\n",
    "    Template for SVAMP-specific extraction fixing script\n",
    "    (Similar to GSM8K hybrid approach but adapted for SVAMP characteristics)\n",
    "    \"\"\"\n",
    "    \n",
    "    template_code = '''\n",
    "# SVAMP HYBRID EXTRACTION FIX (Adapted from GSM8K approach)\n",
    "import json\n",
    "import re\n",
    "\n",
    "def fix_svamp_extraction(input_file, output_file):\n",
    "    \"\"\"\n",
    "    SVAMP-specific extraction fixing with context awareness\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load SVAMP CoT dataset\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    fixed_count = 0\n",
    "    svamp_method_stats = {\"high_confidence\": 0, \"context_filtered\": 0, \"problem_type_aware\": 0, \"robust_fallback\": 0}\n",
    "    \n",
    "    for entry in data:\n",
    "        original_ans = entry['ans']\n",
    "        cot = entry['cot']\n",
    "        problem_type = entry['type']  # SVAMP has problem types!\n",
    "        body = entry['body']\n",
    "        question = entry['question']\n",
    "        \n",
    "        # SVAMP-SPECIFIC HIGH-CONFIDENCE PATTERNS\n",
    "        high_confidence_patterns = [\n",
    "            # Final answer with context\n",
    "            r\"final answer.*?(?:is|:)\\s*(\\d+)\",\n",
    "            r\"answer.*?(?:is|:)\\s*(\\d+)\",\n",
    "            r\"each group.*?(\\d+)\",  # SVAMP-specific: \"each group has X\"\n",
    "            r\"(\\d+).*?(?:in each|per group|each group)\",  # \"X in each group\"\n",
    "            \n",
    "            # Problem-type specific patterns\n",
    "            r\"total.*?(?:is|=)\\s*(\\d+)\" if \"Addition\" in problem_type else None,\n",
    "            r\"(?:left|remaining).*?(\\d+)\" if \"Subtraction\" in problem_type else None,\n",
    "            r\"each.*?(?:group|item).*?(\\d+)\" if \"Division\" in problem_type else None,\n",
    "        ]\n",
    "        \n",
    "        # Remove None patterns\n",
    "        high_confidence_patterns = [p for p in high_confidence_patterns if p]\n",
    "        \n",
    "        new_ans = None\n",
    "        method_used = None\n",
    "        \n",
    "        # Method 1: High-confidence SVAMP patterns\n",
    "        for pattern in high_confidence_patterns:\n",
    "            match = re.search(pattern, cot, re.IGNORECASE)\n",
    "            if match:\n",
    "                candidate = match.group(1)\n",
    "                new_ans = candidate\n",
    "                method_used = \"high_confidence\"\n",
    "                break\n",
    "        \n",
    "        # Method 2: Context-filtered extraction (avoid problem setup numbers)\n",
    "        if not new_ans:\n",
    "            # Get original problem numbers to avoid\n",
    "            problem_numbers = set(re.findall(r'\\d+', body))\n",
    "            \n",
    "            # Find numbers in final lines, avoiding problem setup\n",
    "            lines = cot.split('\\\\n')\n",
    "            for line in reversed(lines[-3:]):  # Last 3 lines\n",
    "                numbers = re.findall(r'(\\d+)', line)\n",
    "                for num in numbers:\n",
    "                    if num not in problem_numbers or len(num) >= 3:  # Avoid setup numbers or use large numbers\n",
    "                        new_ans = num\n",
    "                        method_used = \"context_filtered\"\n",
    "                        break\n",
    "                if new_ans:\n",
    "                    break\n",
    "        \n",
    "        # Method 3: Problem-type aware extraction\n",
    "        if not new_ans:\n",
    "            if problem_type == \"Common-Division\":\n",
    "                # Look for division results\n",
    "                div_pattern = r'(\\d+)\\s*/\\s*(\\d+)\\s*=\\s*(\\d+)'\n",
    "                match = re.search(div_pattern, cot)\n",
    "                if match:\n",
    "                    new_ans = match.group(3)  # Division result\n",
    "                    method_used = \"problem_type_aware\"\n",
    "        \n",
    "        # Method 4: Robust fallback (same as GSM8K)\n",
    "        if not new_ans:\n",
    "            # Use the proven GSM8K robust approach\n",
    "            lines = cot.split('\\\\n')\n",
    "            for line in reversed(lines):\n",
    "                numbers = re.findall(r'(\\d+)', line)\n",
    "                if numbers:\n",
    "                    large_numbers = [n for n in numbers if len(n) >= 3]\n",
    "                    new_ans = max(large_numbers, key=int) if large_numbers else max(numbers, key=int)\n",
    "                    method_used = \"robust_fallback\"\n",
    "                    break\n",
    "        \n",
    "        # Apply fix if different\n",
    "        if new_ans and new_ans != original_ans:\n",
    "            entry['ans'] = new_ans\n",
    "            fixed_count += 1\n",
    "        \n",
    "        # Track method statistics\n",
    "        if method_used:\n",
    "            svamp_method_stats[method_used] += 1\n",
    "    \n",
    "    # Save fixed data\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # Validation\n",
    "    correct_count = sum(1 for entry in data if entry['ans'] == entry['gold'])\n",
    "    accuracy = correct_count / len(data) * 100\n",
    "    \n",
    "    print(f\"SVAMP Extraction Fix Results:\")\n",
    "    print(f\"  Fixed: {fixed_count} samples\")\n",
    "    print(f\"  Accuracy: {accuracy:.1f}% ({correct_count}/{len(data)})\")\n",
    "    print(f\"  Method breakdown: {svamp_method_stats}\")\n",
    "    \n",
    "    return data\n",
    "    '''\n",
    "    \n",
    "    return template_code\n",
    "\n",
    "# Display the template\n",
    "template = create_svamp_extraction_fix()\n",
    "print(\"📄 SVAMP EXTRACTION FIX TEMPLATE CREATED\")\n",
    "print(\"   Key Features:\")\n",
    "print(\"   • Problem-type awareness (Division, Addition, Subtraction)\")\n",
    "print(\"   • Context filtering (avoids problem setup numbers)\")\n",
    "print(\"   • SVAMP-specific patterns ('each group', 'per item')\")\n",
    "print(\"   • Fallback to proven GSM8K robust method\")\n",
    "print(\"\\n💾 To use: Create fix_svamp_extraction.py with this template after generation\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7a506",
   "metadata": {},
   "source": [
    "# 📚 SVAMP Chain-of-Thought Generation Setup\n",
    "\n",
    "## 🎯 **What We've Created:**\n",
    "\n",
    "### **1. SVAMP-Specific CoT Function**\n",
    "- **`generate_cot_svamp()`** - Adapted for SVAMP's Body + Question format\n",
    "- **4 Prompt Types Available:**\n",
    "  - `context_aware` ⭐ **RECOMMENDED** \n",
    "  - `story_focused`\n",
    "  - `structured_reasoning` \n",
    "  - `default`\n",
    "\n",
    "### **2. Dataset Structure**\n",
    "- **Source:** `ChilleD/SVAMP` (700 train + 300 test samples)\n",
    "- **Using:** Train split (700 problems) \n",
    "- **Output Format:** JSON with fields: `id`, `body`, `question`, `cot`, `ans`, `gold`, `equation`, `type`, `domain`, `prompt_type`\n",
    "\n",
    "### **3. Generation Configuration**\n",
    "```python\n",
    "SVAMP_TOTAL_SAMPLES = 700        # All training samples\n",
    "SVAMP_CHECKPOINT_EVERY = 20      # Save every 20 samples  \n",
    "SVAMP_CHECKPOINT_FILE = \"cot_svamp_checkpoint.json\"\n",
    "```\n",
    "\n",
    "## 🏆 **Why 'context_aware' Prompt is Recommended:**\n",
    "\n",
    "**SVAMP Problems Structure:**\n",
    "- **Body**: \"There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups\"\n",
    "- **Question**: \"How big is each group of bananas?\"\n",
    "\n",
    "**Why Context-Aware Works Better:**\n",
    "1. **Natural Problem Structure** - Mirrors how humans read SVAMP problems\n",
    "2. **Context Separation** - Explicitly guides model to understand setup first, then question\n",
    "3. **Reduced Confusion** - Prevents model from missing key information in long context\n",
    "4. **Better Focus** - Directs attention to what specifically needs to be solved\n",
    "\n",
    "## 🚀 **Next Steps:**\n",
    "1. **Test First** - Uncomment test lines to verify one sample\n",
    "2. **Run Generation** - Execute the generation loop for all 700 samples\n",
    "3. **Monitor Progress** - Checkpoints save every 20 samples\n",
    "4. **Apply Extraction Fixes** - Use similar hybrid extraction approach as GSM8K if needed\n",
    "\n",
    "**Estimated Time:** ~47 minutes (700 samples × 4 seconds + processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48cc1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b4127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b45389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2798ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LOADING STRATEGYQA DATASET...\n",
      "==================================================\n",
      "📊 STRATEGYQA DATASET ANALYSIS:\n",
      "Total samples: 1603\n",
      "Sample keys: ['qid', 'term', 'description', 'question', 'answer', 'facts']\n",
      "\n",
      "🔍 FIRST SAMPLE STRUCTURE:\n",
      "   qid: 4fd64bb6ce5b78ab20b6\n",
      "   term: Mixed martial arts\n",
      "   description: full contact combat sport\n",
      "   question: Is Mixed martial arts totally original from Roman Colosseum games?\n",
      "   answer: False\n",
      "   facts: Mixed Martial arts in the UFC takes place in an enclosed structure called The Octagon. The Roman Col...\n",
      "\n",
      "✅ STRATEGYQA DATASET LOADED SUCCESSFULLY!\n",
      "   • Total samples: 1603\n",
      "   • Contains: term, description, question, facts, answer\n",
      "   • Answer format: True/False boolean responses\n",
      "   • Ready for Chain-of-Thought generation\n",
      "📊 STRATEGYQA DATASET ANALYSIS:\n",
      "Total samples: 1603\n",
      "Sample keys: ['qid', 'term', 'description', 'question', 'answer', 'facts']\n",
      "\n",
      "🔍 FIRST SAMPLE STRUCTURE:\n",
      "   qid: 4fd64bb6ce5b78ab20b6\n",
      "   term: Mixed martial arts\n",
      "   description: full contact combat sport\n",
      "   question: Is Mixed martial arts totally original from Roman Colosseum games?\n",
      "   answer: False\n",
      "   facts: Mixed Martial arts in the UFC takes place in an enclosed structure called The Octagon. The Roman Col...\n",
      "\n",
      "✅ STRATEGYQA DATASET LOADED SUCCESSFULLY!\n",
      "   • Total samples: 1603\n",
      "   • Contains: term, description, question, facts, answer\n",
      "   • Answer format: True/False boolean responses\n",
      "   • Ready for Chain-of-Thought generation\n"
     ]
    }
   ],
   "source": [
    "# STRATEGYQA DATASET LOADING AND ANALYSIS\n",
    "print(\"🔍 LOADING STRATEGYQA DATASET...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load StrategyQA dataset\n",
    "strategyqa_ds = load_dataset(\"ChilleD/StrategyQA\", split=\"train\")\n",
    "\n",
    "print(f\"📊 STRATEGYQA DATASET ANALYSIS:\")\n",
    "print(f\"Total samples: {len(strategyqa_ds)}\")\n",
    "print(f\"Sample keys: {list(strategyqa_ds[0].keys())}\")\n",
    "\n",
    "# Show first sample\n",
    "sample = strategyqa_ds[0]\n",
    "print(f\"\\n🔍 FIRST SAMPLE STRUCTURE:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str):\n",
    "        preview = value[:100] + \"...\" if len(str(value)) > 100 else value\n",
    "    else:\n",
    "        preview = str(value)\n",
    "    print(f\"   {key}: {preview}\")\n",
    "\n",
    "print(f\"\\n✅ STRATEGYQA DATASET LOADED SUCCESSFULLY!\")\n",
    "print(f\"   • Total samples: {len(strategyqa_ds)}\")\n",
    "print(f\"   • Contains: term, description, question, facts, answer\")\n",
    "print(f\"   • Answer format: True/False boolean responses\")\n",
    "print(f\"   • Ready for Chain-of-Thought generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0fcc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STRATEGYQA CoT generation function created!\n",
      "\n",
      "📋 AVAILABLE PROMPT TYPES:\n",
      "1. 'comprehensive' (RECOMMENDED) - Includes term, description, facts, and question\n",
      "2. 'question_focused' - Focus on question with background context\n",
      "3. 'minimal' - Question + facts only (not recommended)\n",
      "4. 'default' - Simple approach\n",
      "\n",
      "🎯 RECOMMENDED CHOICE: 'comprehensive'\n",
      "   REASON: StrategyQA questions require external knowledge.\n",
      "   This prompt type provides:\n",
      "   • Complete context about the key term/concept\n",
      "   • Detailed supporting facts for reasoning\n",
      "   • Clear question structure for yes/no decision\n",
      "   • Best performance expected for complex reasoning tasks\n"
     ]
    }
   ],
   "source": [
    "# STRATEGYQA CoT GENERATION FUNCTION\n",
    "def generate_cot_strategyqa(sample, prompt_type=\"comprehensive\"):\n",
    "    \"\"\"\n",
    "    Enhanced CoT generation specifically designed for StrategyQA dataset\n",
    "    \n",
    "    StrategyQA requires reasoning with external knowledge, using:\n",
    "    - term: Key concept\n",
    "    - description: Definition of the concept  \n",
    "    - facts: Supporting evidence/context\n",
    "    - question: Yes/No question to answer\n",
    "    \"\"\"\n",
    "    \n",
    "    if prompt_type == \"comprehensive\":\n",
    "        # RECOMMENDED: Include all context for best reasoning\n",
    "        prompt = f\"\"\"Please solve this yes/no question step by step with clear reasoning.\n",
    "\n",
    "**Context:**\n",
    "Term: {sample['term']}\n",
    "Description: {sample['description']}\n",
    "\n",
    "**Supporting Facts:**\n",
    "{sample['facts']}\n",
    "\n",
    "**Question:** {sample['question']}\n",
    "\n",
    "Please provide a step-by-step chain-of-thought analysis and conclude with a clear yes/no answer.\"\"\"\n",
    "\n",
    "    elif prompt_type == \"question_focused\":\n",
    "        # Alternative: Focus mainly on question + facts\n",
    "        prompt = f\"\"\"Please solve this yes/no question step by step with clear reasoning.\n",
    "\n",
    "**Background:** {sample['term']} - {sample['description']}\n",
    "\n",
    "**Key Facts:**\n",
    "{sample['facts']}\n",
    "\n",
    "**Question:** {sample['question']}\n",
    "\n",
    "Please analyze this step by step and provide a clear yes/no answer.\"\"\"\n",
    "\n",
    "    elif prompt_type == \"minimal\":\n",
    "        # Minimal approach (not recommended for StrategyQA)\n",
    "        prompt = f\"\"\"Please solve this step by step:\n",
    "\n",
    "**Question:** {sample['question']}\n",
    "\n",
    "**Relevant Facts:**\n",
    "{sample['facts']}\n",
    "\n",
    "Provide step-by-step reasoning and a clear yes/no answer.\"\"\"\n",
    "\n",
    "    else:  # default\n",
    "        prompt = f\"Question: {sample['question']}\\nLet me think step by step and provide a yes/no answer.\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        trace = response.text\n",
    "        \n",
    "        # Extract Yes/No answer from CoT\n",
    "        # StrategyQA answers are boolean (True/False)\n",
    "        \n",
    "        # Method 1: Look for explicit yes/no patterns\n",
    "        yes_no_patterns = [\n",
    "            r'(?:final\\s+)?(?:answer|conclusion)[\\s:]*(?:is\\s+)?(?:\\*\\*)?(?:yes|no)(?:\\*\\*)?',\n",
    "            r'(?:the\\s+)?answer[\\s:]+(?:\\*\\*)?(?:yes|no)(?:\\*\\*)?',\n",
    "            r'(?:\\*\\*)?(?:yes|no)(?:\\*\\*)?(?:\\s*[\\.!])?$',  # Yes/No at end of line\n",
    "            r'(?:therefore|so|thus)[\\s,]*(?:the\\s+answer\\s+is\\s+)?(?:\\*\\*)?(?:yes|no)(?:\\*\\*)?',\n",
    "        ]\n",
    "        \n",
    "        extracted_answer = None\n",
    "        for pattern in yes_no_patterns:\n",
    "            matches = re.findall(pattern, trace, re.IGNORECASE)\n",
    "            if matches:\n",
    "                # Get the last match and extract yes/no\n",
    "                last_match = matches[-1].lower()\n",
    "                if 'yes' in last_match:\n",
    "                    extracted_answer = \"True\"\n",
    "                elif 'no' in last_match:\n",
    "                    extracted_answer = \"False\"\n",
    "                break\n",
    "        \n",
    "        # Method 2: Fallback - look for yes/no anywhere in last few lines\n",
    "        if not extracted_answer:\n",
    "            lines = trace.split('\\n')\n",
    "            for line in reversed(lines[-5:]):  # Check last 5 lines\n",
    "                line_lower = line.lower()\n",
    "                if 'yes' in line_lower and 'no' not in line_lower:\n",
    "                    extracted_answer = \"True\"\n",
    "                    break\n",
    "                elif 'no' in line_lower and 'yes' not in line_lower:\n",
    "                    extracted_answer = \"False\"\n",
    "                    break\n",
    "        \n",
    "        # Convert gold answer to string for consistency\n",
    "        gold_answer = str(sample['answer'])\n",
    "        \n",
    "        return {\n",
    "            \"qid\": sample['qid'],\n",
    "            \"term\": sample['term'],\n",
    "            \"description\": sample['description'],\n",
    "            \"question\": sample['question'],\n",
    "            \"facts\": sample['facts'],\n",
    "            \"cot\": trace,\n",
    "            \"ans\": extracted_answer if extracted_answer else gold_answer,  # Fallback to gold if extraction fails\n",
    "            \"gold\": gold_answer,\n",
    "            \"domain\": \"strategyqa\",\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content for StrategyQA problem: {sample['qid']}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return {\n",
    "            \"qid\": sample['qid'],\n",
    "            \"term\": sample['term'],\n",
    "            \"description\": sample['description'],\n",
    "            \"question\": sample['question'],\n",
    "            \"facts\": sample['facts'],\n",
    "            \"cot\": f\"Error: {e}\",\n",
    "            \"ans\": str(sample['answer']),\n",
    "            \"gold\": str(sample['answer']),\n",
    "            \"domain\": \"strategyqa\",\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "\n",
    "print(\"✅ STRATEGYQA CoT generation function created!\")\n",
    "print(\"\\n📋 AVAILABLE PROMPT TYPES:\")\n",
    "print(\"1. 'comprehensive' (RECOMMENDED) - Includes term, description, facts, and question\")\n",
    "print(\"2. 'question_focused' - Focus on question with background context\")\n",
    "print(\"3. 'minimal' - Question + facts only (not recommended)\")\n",
    "print(\"4. 'default' - Simple approach\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED CHOICE: 'comprehensive'\")\n",
    "print(\"   REASON: StrategyQA questions require external knowledge.\")\n",
    "print(\"   This prompt type provides:\")\n",
    "print(\"   • Complete context about the key term/concept\")\n",
    "print(\"   • Detailed supporting facts for reasoning\")\n",
    "print(\"   • Clear question structure for yes/no decision\")\n",
    "print(\"   • Best performance expected for complex reasoning tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d283f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 PREPARING STRATEGYQA DATASET FOR COT GENERATION...\n",
      "📊 StrategyQA Dataset prepared: 1603 problems\n",
      "   Answer distribution (first 100): {'False': 58, 'True': 42}\n",
      "📁 FOUND CHECKPOINT: 1000 samples already completed\n",
      "📊 Progress: 1000/1500 (66.7%)\n",
      "✅ Resuming from sample #1001\n",
      "🚀 Remaining: 500 samples\n",
      "\n",
      "🚀 STRATEGYQA COT GENERATION STATUS:\n",
      "   • Total target: 1500\n",
      "   • Already completed: 1000\n",
      "   • Remaining: 500\n",
      "   • Checkpoint every: 25 samples\n",
      "   • Recommended prompt: 'comprehensive'\n",
      "   • Rate limit: 4 seconds between requests\n",
      "   • Estimated time: ~33 minutes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# STRATEGYQA DATASET PREPARATION AND CONFIGURATION\n",
    "print(\"🔄 PREPARING STRATEGYQA DATASET FOR COT GENERATION...\")\n",
    "\n",
    "# Prepare StrategyQA questions (you can select subset for testing)\n",
    "strategyqa_train = strategyqa_ds\n",
    "strategyqa_questions = []\n",
    "\n",
    "for sample in strategyqa_train:\n",
    "    strategyqa_questions.append({\n",
    "        'qid': sample['qid'],\n",
    "        'term': sample['term'],\n",
    "        'description': sample['description'],\n",
    "        'question': sample['question'],\n",
    "        'facts': sample['facts'],\n",
    "        'answer': sample['answer']\n",
    "    })\n",
    "\n",
    "print(f\"📊 StrategyQA Dataset prepared: {len(strategyqa_questions)} problems\")\n",
    "\n",
    "# Show sample of different answer types\n",
    "answer_distribution = {}\n",
    "for q in strategyqa_questions[:100]:  # Check first 100\n",
    "    ans = str(q['answer'])\n",
    "    answer_distribution[ans] = answer_distribution.get(ans, 0) + 1\n",
    "print(f\"   Answer distribution (first 100): {answer_distribution}\")\n",
    "\n",
    "# STRATEGYQA GENERATION CONFIGURATION\n",
    "STRATEGYQA_TOTAL_SAMPLES = 1500  # USER REQUESTED: 1500 samples\n",
    "STRATEGYQA_CHECKPOINT_EVERY = 25  # Save progress every N samples\n",
    "STRATEGYQA_CHECKPOINT_FILE = \"cot_strategyqa_checkpoint.json\"\n",
    "\n",
    "# Load existing progress if any - FIXED LOGIC\n",
    "strategyqa_dataset = []\n",
    "strategyqa_start_index = 0\n",
    "\n",
    "if os.path.exists(STRATEGYQA_CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(STRATEGYQA_CHECKPOINT_FILE, 'r') as f:\n",
    "            strategyqa_dataset = json.load(f)\n",
    "        strategyqa_start_index = len(strategyqa_dataset)\n",
    "        print(f\"📁 FOUND CHECKPOINT: {strategyqa_start_index} samples already completed\")\n",
    "        print(f\"📊 Progress: {strategyqa_start_index}/{STRATEGYQA_TOTAL_SAMPLES} ({strategyqa_start_index/STRATEGYQA_TOTAL_SAMPLES*100:.1f}%)\")\n",
    "        \n",
    "        if strategyqa_start_index > 0:\n",
    "            print(f\"✅ Resuming from sample #{strategyqa_start_index + 1}\")\n",
    "            print(f\"🚀 Remaining: {STRATEGYQA_TOTAL_SAMPLES - strategyqa_start_index} samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading StrategyQA checkpoint: {e}\")\n",
    "        print(\"🔄 Starting fresh...\")\n",
    "        strategyqa_dataset = []\n",
    "        strategyqa_start_index = 0\n",
    "else:\n",
    "    print(\"📄 No checkpoint found - starting from beginning\")\n",
    "\n",
    "print(f\"\\n🚀 STRATEGYQA COT GENERATION STATUS:\")\n",
    "print(f\"   • Total target: {STRATEGYQA_TOTAL_SAMPLES}\")\n",
    "print(f\"   • Already completed: {strategyqa_start_index}\")\n",
    "print(f\"   • Remaining: {STRATEGYQA_TOTAL_SAMPLES - strategyqa_start_index}\")\n",
    "print(f\"   • Checkpoint every: {STRATEGYQA_CHECKPOINT_EVERY} samples\")\n",
    "print(f\"   • Recommended prompt: 'comprehensive'\")\n",
    "print(f\"   • Rate limit: 4 seconds between requests\")\n",
    "print(f\"   • Estimated time: ~{((STRATEGYQA_TOTAL_SAMPLES - strategyqa_start_index) * 4) // 60} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6194468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGYQA COT GENERATION LOOP - FIXED CHECKPOINT RESUMPTION\n",
    "if strategyqa_start_index >= STRATEGYQA_TOTAL_SAMPLES:\n",
    "    print(\"🎉 StrategyQA CoT generation already completed! All samples generated.\")\n",
    "    print(f\"\udcc1 Final file should be: cot_strategyqa_final_{len(strategyqa_dataset)}.json\")\n",
    "else:\n",
    "    remaining_samples = STRATEGYQA_TOTAL_SAMPLES - strategyqa_start_index\n",
    "    print(f\"🚀 Generating {remaining_samples} new StrategyQA CoT samples...\")\n",
    "    print(f\"\udccd Starting from sample #{strategyqa_start_index + 1}\")\n",
    "    print(f\"\ud83d💾 Checkpoints every {STRATEGYQA_CHECKPOINT_EVERY} samples\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate remaining samples - FIXED: Use correct slice and counter\n",
    "    for i, sample in enumerate(tqdm(strategyqa_questions[strategyqa_start_index:STRATEGYQA_TOTAL_SAMPLES], \n",
    "                                  desc=f\"Processing StrategyQA {strategyqa_start_index+1}-{STRATEGYQA_TOTAL_SAMPLES}\", \n",
    "                                  unit=\"problem\")):\n",
    "        \n",
    "        # Use comprehensive prompt for best reasoning with full context\n",
    "        entry = generate_cot_strategyqa(sample, \"comprehensive\")\n",
    "        strategyqa_dataset.append(entry)\n",
    "        \n",
    "        # Checkpoint saving with Windows-compatible atomic write\n",
    "        if len(strategyqa_dataset) % STRATEGYQA_CHECKPOINT_EVERY == 0:\n",
    "            temp_file = f\"temp_{STRATEGYQA_CHECKPOINT_FILE}\"\n",
    "            with open(temp_file, \"w\") as f:\n",
    "                json.dump(strategyqa_dataset, f, indent=2)\n",
    "            \n",
    "            # Windows-compatible atomic write\n",
    "            if os.path.exists(STRATEGYQA_CHECKPOINT_FILE):\n",
    "                os.remove(STRATEGYQA_CHECKPOINT_FILE)\n",
    "            os.rename(temp_file, STRATEGYQA_CHECKPOINT_FILE)\n",
    "            print(f\"💾 StrategyQA Checkpoint: {len(strategyqa_dataset)}/{STRATEGYQA_TOTAL_SAMPLES} samples saved\")\n",
    "        \n",
    "        time.sleep(4)  # Rate limit delay\n",
    "\n",
    "    # Final save\n",
    "    with open(STRATEGYQA_CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(strategyqa_dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ StrategyQA Generation complete! {len(strategyqa_dataset)} samples saved to {STRATEGYQA_CHECKPOINT_FILE}\")\n",
    "\n",
    "    # Create final numbered file\n",
    "    strategyqa_final_file = f\"cot_strategyqa_final_{len(strategyqa_dataset)}.json\"\n",
    "    with open(strategyqa_final_file, \"w\") as f:\n",
    "        json.dump(strategyqa_dataset, f, indent=2)\n",
    "    print(f\"🏁 Final StrategyQA dataset: {strategyqa_final_file}\")\n",
    "    \n",
    "    # Show sample of generated data\n",
    "    if strategyqa_dataset:\n",
    "        print(f\"\\n📋 SAMPLE GENERATED STRATEGYQA COT:\")\n",
    "        sample_entry = strategyqa_dataset[0]\n",
    "        print(f\"   QID: {sample_entry['qid']}\")\n",
    "        print(f\"   Term: {sample_entry['term']}\")\n",
    "        print(f\"   Question: {sample_entry['question'][:100]}...\")\n",
    "        print(f\"   Generated Answer: {sample_entry['ans']}\")\n",
    "        print(f\"   Gold Answer: {sample_entry['gold']}\")\n",
    "        print(f\"   CoT Length: {len(sample_entry['cot'])} characters\")\n",
    "        \n",
    "        # Check accuracy on generated samples\n",
    "        correct = sum(1 for entry in strategyqa_dataset if entry['ans'] == entry['gold'])\n",
    "        accuracy = correct / len(strategyqa_dataset) * 100\n",
    "        print(f\"   Current Accuracy: {accuracy:.1f}% ({correct}/{len(strategyqa_dataset)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Demonstrate StrategyQA CoT Generation\n",
    "print(\"🧪 TESTING STRATEGYQA CoT GENERATION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with first StrategyQA problem\n",
    "test_sample = strategyqa_questions[0]\n",
    "print(f\"📋 TEST PROBLEM:\")\n",
    "print(f\"   QID: {test_sample['qid']}\")\n",
    "print(f\"   Term: {test_sample['term']}\")\n",
    "print(f\"   Description: {test_sample['description']}\")\n",
    "print(f\"   Question: {test_sample['question']}\")\n",
    "print(f\"   Facts: {test_sample['facts'][:200]}...\")\n",
    "print(f\"   Gold Answer: {test_sample['answer']}\")\n",
    "\n",
    "print(f\"\\n🤖 GENERATING CoT WITH 'comprehensive' PROMPT...\")\n",
    "# Uncomment the line below to test (will use 1 API call)\n",
    "# result = generate_cot_strategyqa(test_sample, \"comprehensive\")\n",
    "# print(f\"Generated CoT: {result['cot'][:300]}...\")\n",
    "# print(f\"Extracted Answer: {result['ans']}\")\n",
    "# print(f\"Gold Answer: {result['gold']}\")\n",
    "# print(f\"Correct: {'✅' if result['ans'] == result['gold'] else '❌'}\")\n",
    "\n",
    "print(f\"\\n⚡ TO TEST: Uncomment the lines above (uses 1 API call)\")\n",
    "print(f\"⚡ TO START FULL GENERATION: Run the generation loop cell\")\n",
    "print(f\"📊 This will generate {STRATEGYQA_TOTAL_SAMPLES} CoT samples for StrategyQA dataset\")\n",
    "print(f\"⏱️  Estimated time: ~{(STRATEGYQA_TOTAL_SAMPLES * 4) // 60} minutes\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c90eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 CHECKING STRATEGYQA CHECKPOINT STATUS:\n",
      "==================================================\n",
      "✅ CHECKPOINT FILE FOUND!\n",
      "📊 Current Status:\n",
      "   • Completed samples: 1000\n",
      "   • Target samples: 1500\n",
      "   • Remaining samples: 500\n",
      "   • Progress: 66.7%\n",
      "🚀 READY TO RESUME from sample #1001\n",
      "⏱️  Estimated time: ~33 minutes\n",
      "\n",
      "📋 SAMPLE FROM EXISTING DATA:\n",
      "   QID: 4fd64bb6ce5b78ab20b6\n",
      "   Question: Is Mixed martial arts totally original from Roman Colosseum games?...\n",
      "   Answer: False\n",
      "   Gold: False\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# CHECKPOINT STATUS CHECK - Run this first to see current state\n",
    "print(\"🔍 CHECKING STRATEGYQA CHECKPOINT STATUS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "STRATEGYQA_CHECKPOINT_FILE = \"cot_strategyqa_checkpoint.json\"\n",
    "STRATEGYQA_TOTAL_SAMPLES = 1500\n",
    "\n",
    "if os.path.exists(STRATEGYQA_CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(STRATEGYQA_CHECKPOINT_FILE, 'r') as f:\n",
    "            existing_data = json.load(f)\n",
    "        \n",
    "        completed_samples = len(existing_data)\n",
    "        remaining_samples = STRATEGYQA_TOTAL_SAMPLES - completed_samples\n",
    "        \n",
    "        print(f\"✅ CHECKPOINT FILE FOUND!\")\n",
    "        print(f\"📊 Current Status:\")\n",
    "        print(f\"   • Completed samples: {completed_samples}\")\n",
    "        print(f\"   • Target samples: {STRATEGYQA_TOTAL_SAMPLES}\")\n",
    "        print(f\"   • Remaining samples: {remaining_samples}\")\n",
    "        print(f\"   • Progress: {completed_samples/STRATEGYQA_TOTAL_SAMPLES*100:.1f}%\")\n",
    "        \n",
    "        if completed_samples >= STRATEGYQA_TOTAL_SAMPLES:\n",
    "            print(f\"🎉 GENERATION ALREADY COMPLETE!\")\n",
    "            print(f\"📁 Checkpoint file: {STRATEGYQA_CHECKPOINT_FILE}\")\n",
    "            \n",
    "            # Check if final file exists\n",
    "            final_file = f\"cot_strategyqa_final_{completed_samples}.json\"\n",
    "            if os.path.exists(final_file):\n",
    "                print(f\"📁 Final file exists: {final_file}\")\n",
    "            else:\n",
    "                print(f\"⚠️  Final file missing - will be created when you run generation cell\")\n",
    "        else:\n",
    "            print(f\"🚀 READY TO RESUME from sample #{completed_samples + 1}\")\n",
    "            print(f\"⏱️  Estimated time: ~{(remaining_samples * 4) // 60} minutes\")\n",
    "        \n",
    "        # Show sample of existing data\n",
    "        if existing_data:\n",
    "            print(f\"\\n📋 SAMPLE FROM EXISTING DATA:\")\n",
    "            sample = existing_data[0]\n",
    "            print(f\"   QID: {sample.get('qid', 'N/A')}\")\n",
    "            print(f\"   Question: {sample.get('question', 'N/A')[:80]}...\")\n",
    "            print(f\"   Answer: {sample.get('ans', 'N/A')}\")\n",
    "            print(f\"   Gold: {sample.get('gold', 'N/A')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR reading checkpoint file: {e}\")\n",
    "        print(f\"🔄 Checkpoint file exists but corrupted - will start fresh\")\n",
    "else:\n",
    "    print(f\"📄 NO CHECKPOINT FILE FOUND\")\n",
    "    print(f\"🚀 Will start from beginning\")\n",
    "    print(f\"⏱️  Estimated time: ~{(STRATEGYQA_TOTAL_SAMPLES * 4) // 60} minutes for all {STRATEGYQA_TOTAL_SAMPLES} samples\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297c59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 SAFER STRATEGYQA GENERATION WITH SMALL BATCHES\n",
      "============================================================\n",
      "📊 CURRENT STATUS:\n",
      "   • Completed: 1000\n",
      "   • Remaining: 500\n",
      "   • Batch size: 10\n",
      "\n",
      "🚀 PROCESSING BATCH OF 10 SAMPLES...\n",
      "==================================================\n",
      "🔄 Processing sample 1001/1500 (QID: 9885d9bb4506cdf4f2cd)\n",
      "   ✅ Generated (Answer: False, Length: 1247 chars)\n",
      "🔄 Processing sample 1002/1500 (QID: b65adb5caa4f7a207879)\n",
      "   ✅ Generated (Answer: False, Length: 1660 chars)\n",
      "🔄 Processing sample 1003/1500 (QID: 42025ba75ec5d0f0f291)\n",
      "   ✅ Generated (Answer: False, Length: 1667 chars)\n",
      "🔄 Processing sample 1004/1500 (QID: 7f8417d42ef9ea1e5a8c)\n",
      "   ✅ Generated (Answer: True, Length: 2007 chars)\n",
      "🔄 Processing sample 1005/1500 (QID: 4ba70839df733c61f9a5)\n",
      "   ✅ Generated (Answer: False, Length: 1321 chars)\n",
      "   💾 Checkpoint saved: 1005 samples\n",
      "🔄 Processing sample 1006/1500 (QID: 80ca095e38ab73b908ff)\n",
      "   ✅ Generated (Answer: False, Length: 2711 chars)\n",
      "🔄 Processing sample 1007/1500 (QID: 5d4ae0455b8641ff0c03)\n",
      "   ✅ Generated (Answer: True, Length: 2855 chars)\n",
      "🔄 Processing sample 1008/1500 (QID: ccb62fcd2b202d737f79)\n",
      "   ✅ Generated (Answer: False, Length: 1846 chars)\n",
      "🔄 Processing sample 1009/1500 (QID: f315b85273989097eb91)\n",
      "   ✅ Generated (Answer: True, Length: 1718 chars)\n",
      "🔄 Processing sample 1010/1500 (QID: 0357ef6f6b33d7c077d3)\n",
      "   ✅ Generated (Answer: False, Length: 1769 chars)\n",
      "   💾 Checkpoint saved: 1010 samples\n",
      "\n",
      "✅ BATCH COMPLETE!\n",
      "   • Processed: 10 samples\n",
      "   • Total completed: 1010\n",
      "   • Remaining: 490\n",
      "🔄 Run this cell again to process the next batch\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SAFER STRATEGYQA GENERATION - SMALL BATCH APPROACH\n",
    "print(\"🔧 SAFER STRATEGYQA GENERATION WITH SMALL BATCHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import gc  # Garbage collection to help with memory\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configuration for safer generation\n",
    "BATCH_SIZE = 10  # Process only 10 samples at a time\n",
    "STRATEGYQA_CHECKPOINT_FILE = \"cot_strategyqa_checkpoint.json\"\n",
    "STRATEGYQA_TOTAL_SAMPLES = 1500\n",
    "\n",
    "# Check current status\n",
    "if os.path.exists(STRATEGYQA_CHECKPOINT_FILE):\n",
    "    with open(STRATEGYQA_CHECKPOINT_FILE, 'r') as f:\n",
    "        strategyqa_dataset = json.load(f)\n",
    "    strategyqa_start_index = len(strategyqa_dataset)\n",
    "else:\n",
    "    strategyqa_dataset = []\n",
    "    strategyqa_start_index = 0\n",
    "\n",
    "remaining_samples = STRATEGYQA_TOTAL_SAMPLES - strategyqa_start_index\n",
    "\n",
    "print(f\"📊 CURRENT STATUS:\")\n",
    "print(f\"   • Completed: {strategyqa_start_index}\")\n",
    "print(f\"   • Remaining: {remaining_samples}\")\n",
    "print(f\"   • Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "if remaining_samples <= 0:\n",
    "    print(\"🎉 Already complete!\")\n",
    "else:\n",
    "    print(f\"\\n🚀 PROCESSING BATCH OF {min(BATCH_SIZE, remaining_samples)} SAMPLES...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    batch_end = min(strategyqa_start_index + BATCH_SIZE, STRATEGYQA_TOTAL_SAMPLES)\n",
    "    batch_samples = strategyqa_questions[strategyqa_start_index:batch_end]\n",
    "    \n",
    "    # Process batch with individual progress\n",
    "    for i, sample in enumerate(batch_samples):\n",
    "        current_sample = strategyqa_start_index + i + 1\n",
    "        print(f\"🔄 Processing sample {current_sample}/{STRATEGYQA_TOTAL_SAMPLES} (QID: {sample['qid']})\")\n",
    "        \n",
    "        try:\n",
    "            # Generate with timeout protection\n",
    "            entry = generate_cot_strategyqa(sample, \"comprehensive\")\n",
    "            strategyqa_dataset.append(entry)\n",
    "            \n",
    "            print(f\"   ✅ Generated (Answer: {entry['ans']}, Length: {len(entry['cot'])} chars)\")\n",
    "            \n",
    "            # Immediate checkpoint save for safety\n",
    "            if len(strategyqa_dataset) % 5 == 0:  # Save every 5 samples in batch mode\n",
    "                with open(STRATEGYQA_CHECKPOINT_FILE, \"w\") as f:\n",
    "                    json.dump(strategyqa_dataset, f, indent=2)\n",
    "                print(f\"   💾 Checkpoint saved: {len(strategyqa_dataset)} samples\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            gc.collect()\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ ERROR on sample {current_sample}: {e}\")\n",
    "            # Continue with next sample rather than crashing\n",
    "            continue\n",
    "    \n",
    "    # Final save\n",
    "    with open(STRATEGYQA_CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(strategyqa_dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ BATCH COMPLETE!\")\n",
    "    print(f\"   • Processed: {len(batch_samples)} samples\")\n",
    "    print(f\"   • Total completed: {len(strategyqa_dataset)}\")\n",
    "    print(f\"   • Remaining: {STRATEGYQA_TOTAL_SAMPLES - len(strategyqa_dataset)}\")\n",
    "    \n",
    "    if len(strategyqa_dataset) >= STRATEGYQA_TOTAL_SAMPLES:\n",
    "        print(\"🎉 FULL GENERATION COMPLETE!\")\n",
    "        # Create final file\n",
    "        final_file = f\"cot_strategyqa_final_{len(strategyqa_dataset)}.json\"\n",
    "        with open(final_file, \"w\") as f:\n",
    "            json.dump(strategyqa_dataset, f, indent=2)\n",
    "        print(f\"🏁 Final dataset saved: {final_file}\")\n",
    "    else:\n",
    "        print(\"🔄 Run this cell again to process the next batch\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e2028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 COMPLETING REMAINING STRATEGYQA SAMPLES\n",
      "============================================================\n",
      "🚀 Generating 490 remaining StrategyQA CoT samples...\n",
      "📍 Starting from sample #1011\n",
      "💾 Checkpoints every 25 samples\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:   3%|▎         | 14/490 [01:21<47:35,  6.00s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1025/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:   8%|▊         | 39/490 [03:47<44:11,  5.88s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1050/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  13%|█▎        | 64/490 [06:18<47:34,  6.70s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1075/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  18%|█▊        | 89/490 [08:44<38:21,  5.74s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1100/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  23%|██▎       | 114/490 [11:14<38:05,  6.08s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1125/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  28%|██▊       | 139/490 [13:46<38:02,  6.50s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1150/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  33%|███▎      | 164/490 [16:17<33:52,  6.23s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1175/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  39%|███▊      | 189/490 [18:49<28:38,  5.71s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1200/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  44%|████▎     | 214/490 [21:18<26:13,  5.70s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1225/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  49%|████▉     | 239/490 [23:46<25:05,  6.00s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1250/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  54%|█████▍    | 264/490 [26:16<23:02,  6.12s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1275/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  59%|█████▉    | 289/490 [28:42<20:32,  6.13s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1300/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  64%|██████▍   | 314/490 [31:25<20:56,  7.14s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1325/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  69%|██████▉   | 339/490 [33:54<14:48,  5.89s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1350/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  74%|███████▍  | 364/490 [36:23<12:38,  6.02s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1375/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  79%|███████▉  | 389/490 [39:08<12:29,  7.42s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1400/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  84%|████████▍ | 414/490 [41:42<07:35,  5.99s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1425/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  90%|████████▉ | 439/490 [44:11<05:08,  6.05s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1450/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500:  95%|█████████▍| 464/490 [46:44<02:35,  5.99s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1475/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500: 100%|█████████▉| 489/490 [49:15<00:05,  5.90s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 StrategyQA Checkpoint: 1500/1500 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StrategyQA 1011-1500: 100%|██████████| 490/490 [49:21<00:00,  6.04s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ StrategyQA Generation complete! 1500 samples saved to cot_strategyqa_checkpoint.json\n",
      "🏁 Final StrategyQA dataset: cot_strategyqa_final_1500.json\n",
      "\n",
      "📋 SAMPLE GENERATED STRATEGYQA COT:\n",
      "   QID: 4fd64bb6ce5b78ab20b6\n",
      "   Term: Mixed martial arts\n",
      "   Question: Is Mixed martial arts totally original from Roman Colosseum games?...\n",
      "   Generated Answer: False\n",
      "   Gold Answer: False\n",
      "   CoT Length: 2988 characters\n",
      "   📊 Final Accuracy: 94.6% (1419/1500)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE REMAINING STRATEGYQA GENERATION (1010 → 1500)\n",
    "print(\"🚀 COMPLETING REMAINING STRATEGYQA SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "STRATEGYQA_CHECKPOINT_FILE = \"cot_strategyqa_checkpoint.json\"\n",
    "STRATEGYQA_TOTAL_SAMPLES = 1500\n",
    "STRATEGYQA_CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Load existing progress\n",
    "if os.path.exists(STRATEGYQA_CHECKPOINT_FILE):\n",
    "    with open(STRATEGYQA_CHECKPOINT_FILE, 'r') as f:\n",
    "        strategyqa_dataset = json.load(f)\n",
    "    strategyqa_start_index = len(strategyqa_dataset)\n",
    "else:\n",
    "    strategyqa_dataset = []\n",
    "    strategyqa_start_index = 0\n",
    "\n",
    "if strategyqa_start_index >= STRATEGYQA_TOTAL_SAMPLES:\n",
    "    print(\"🎉 StrategyQA CoT generation already completed! All samples generated.\")\n",
    "    \n",
    "    # Create final file if it doesn't exist\n",
    "    final_file = f\"cot_strategyqa_final_{len(strategyqa_dataset)}.json\"\n",
    "    if not os.path.exists(final_file):\n",
    "        with open(final_file, \"w\") as f:\n",
    "            json.dump(strategyqa_dataset, f, indent=2)\n",
    "        print(f\"🏁 Final dataset created: {final_file}\")\n",
    "    else:\n",
    "        print(f\"📁 Final file already exists: {final_file}\")\n",
    "else:\n",
    "    remaining_samples = STRATEGYQA_TOTAL_SAMPLES - strategyqa_start_index\n",
    "    print(f\"🚀 Generating {remaining_samples} remaining StrategyQA CoT samples...\")\n",
    "    print(f\"📍 Starting from sample #{strategyqa_start_index + 1}\")\n",
    "    print(f\"💾 Checkpoints every {STRATEGYQA_CHECKPOINT_EVERY} samples\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate remaining samples\n",
    "    for i, sample in enumerate(tqdm(strategyqa_questions[strategyqa_start_index:STRATEGYQA_TOTAL_SAMPLES], \n",
    "                                  desc=f\"Processing StrategyQA {strategyqa_start_index+1}-{STRATEGYQA_TOTAL_SAMPLES}\", \n",
    "                                  unit=\"problem\")):\n",
    "        \n",
    "        # Use comprehensive prompt for best reasoning with full context\n",
    "        entry = generate_cot_strategyqa(sample, \"comprehensive\")\n",
    "        strategyqa_dataset.append(entry)\n",
    "        \n",
    "        # Checkpoint saving with Windows-compatible atomic write\n",
    "        if len(strategyqa_dataset) % STRATEGYQA_CHECKPOINT_EVERY == 0:\n",
    "            temp_file = f\"temp_{STRATEGYQA_CHECKPOINT_FILE}\"\n",
    "            with open(temp_file, \"w\") as f:\n",
    "                json.dump(strategyqa_dataset, f, indent=2)\n",
    "            \n",
    "            # Windows-compatible atomic write\n",
    "            if os.path.exists(STRATEGYQA_CHECKPOINT_FILE):\n",
    "                os.remove(STRATEGYQA_CHECKPOINT_FILE)\n",
    "            os.rename(temp_file, STRATEGYQA_CHECKPOINT_FILE)\n",
    "            print(f\"💾 StrategyQA Checkpoint: {len(strategyqa_dataset)}/{STRATEGYQA_TOTAL_SAMPLES} samples saved\")\n",
    "        \n",
    "        time.sleep(4)  # Rate limit delay\n",
    "\n",
    "    # Final save\n",
    "    with open(STRATEGYQA_CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(strategyqa_dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ StrategyQA Generation complete! {len(strategyqa_dataset)} samples saved to {STRATEGYQA_CHECKPOINT_FILE}\")\n",
    "\n",
    "    # Create final numbered file\n",
    "    strategyqa_final_file = f\"cot_strategyqa_final_{len(strategyqa_dataset)}.json\"\n",
    "    with open(strategyqa_final_file, \"w\") as f:\n",
    "        json.dump(strategyqa_dataset, f, indent=2)\n",
    "    print(f\"🏁 Final StrategyQA dataset: {strategyqa_final_file}\")\n",
    "    \n",
    "    # Show sample of generated data and accuracy\n",
    "    if strategyqa_dataset:\n",
    "        print(f\"\\n📋 SAMPLE GENERATED STRATEGYQA COT:\")\n",
    "        sample_entry = strategyqa_dataset[0]\n",
    "        print(f\"   QID: {sample_entry['qid']}\")\n",
    "        print(f\"   Term: {sample_entry['term']}\")\n",
    "        print(f\"   Question: {sample_entry['question'][:100]}...\")\n",
    "        print(f\"   Generated Answer: {sample_entry['ans']}\")\n",
    "        print(f\"   Gold Answer: {sample_entry['gold']}\")\n",
    "        print(f\"   CoT Length: {len(sample_entry['cot'])} characters\")\n",
    "        \n",
    "        # Check accuracy on all samples\n",
    "        correct = sum(1 for entry in strategyqa_dataset if entry['ans'] == entry['gold'])\n",
    "        accuracy = correct / len(strategyqa_dataset) * 100\n",
    "        print(f\"   📊 Final Accuracy: {accuracy:.1f}% ({correct}/{len(strategyqa_dataset)})\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fec4e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 ADDING 'is_correct' COLUMN TO STRATEGYQA DATASET\n",
      "============================================================\n",
      "📊 Loaded 1500 samples from cot_strategyqa_final_1500.json\n",
      "✅ Successfully added 'is_correct' column!\n",
      "📈 Dataset Statistics:\n",
      "   • Total samples: 1500\n",
      "   • Correct answers: 1419\n",
      "   • Incorrect answers: 81\n",
      "   • Accuracy: 94.60%\n",
      "📁 Updated dataset saved as: cot_strategyqa_final_1500_with_correct.json\n",
      "\n",
      "📋 SAMPLE ENTRIES WITH 'is_correct' COLUMN:\n",
      "\n",
      "Sample 1:\n",
      "   QID: 4fd64bb6ce5b78ab20b6\n",
      "   Question: Is Mixed martial arts totally original from Roman Colosseum games?...\n",
      "   Generated Answer: False\n",
      "   Gold Answer: False\n",
      "   Is Correct: True\n",
      "\n",
      "Sample 2:\n",
      "   QID: f378f856bdaff39cdfa3\n",
      "   Question: Is the cuisine of Hawaii suitable for a vegan?...\n",
      "   Generated Answer: False\n",
      "   Gold Answer: False\n",
      "   Is Correct: True\n",
      "\n",
      "Sample 3:\n",
      "   QID: 4e1b65e81ec09397b26e\n",
      "   Question: Is capturing giant squid in natural habitat impossible with no gear?...\n",
      "   Generated Answer: True\n",
      "   Gold Answer: True\n",
      "   Is Correct: True\n",
      "💾 Backup created: cot_strategyqa_final_1500_backup.json\n",
      "🔄 Original file updated: cot_strategyqa_final_1500.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ADD 'is_correct' COLUMN TO STRATEGYQA DATASET\n",
    "print(\"🔧 ADDING 'is_correct' COLUMN TO STRATEGYQA DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_file = \"cot_strategyqa_final_1500.json\"\n",
    "output_file = \"cot_strategyqa_final_1500_with_correct.json\"\n",
    "\n",
    "if os.path.exists(input_file):\n",
    "    # Load the dataset\n",
    "    with open(input_file, 'r') as f:\n",
    "        strategyqa_data = json.load(f)\n",
    "    \n",
    "    print(f\"📊 Loaded {len(strategyqa_data)} samples from {input_file}\")\n",
    "    \n",
    "    # Add 'is_correct' column\n",
    "    correct_count = 0\n",
    "    for entry in strategyqa_data:\n",
    "        # Compare generated answer with gold answer\n",
    "        is_correct = entry['ans'] == entry['gold']\n",
    "        entry['is_correct'] = is_correct\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(strategyqa_data, f, indent=2)\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    total_samples = len(strategyqa_data)\n",
    "    accuracy = (correct_count / total_samples) * 100\n",
    "    \n",
    "    print(f\"✅ Successfully added 'is_correct' column!\")\n",
    "    print(f\"📈 Dataset Statistics:\")\n",
    "    print(f\"   • Total samples: {total_samples}\")\n",
    "    print(f\"   • Correct answers: {correct_count}\")\n",
    "    print(f\"   • Incorrect answers: {total_samples - correct_count}\")\n",
    "    print(f\"   • Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"📁 Updated dataset saved as: {output_file}\")\n",
    "    \n",
    "    # Show sample entries with the new column\n",
    "    print(f\"\\n📋 SAMPLE ENTRIES WITH 'is_correct' COLUMN:\")\n",
    "    for i, entry in enumerate(strategyqa_data[:3]):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"   QID: {entry['qid']}\")\n",
    "        print(f\"   Question: {entry['question'][:80]}...\")\n",
    "        print(f\"   Generated Answer: {entry['ans']}\")\n",
    "        print(f\"   Gold Answer: {entry['gold']}\")\n",
    "        print(f\"   Is Correct: {entry['is_correct']}\")\n",
    "    \n",
    "    # Also update the original file (backup approach)\n",
    "    backup_file = \"cot_strategyqa_final_1500_backup.json\"\n",
    "    if not os.path.exists(backup_file):\n",
    "        # Create backup of original\n",
    "        with open(input_file, 'r') as f:\n",
    "            original_data = json.load(f)\n",
    "        with open(backup_file, 'w') as f:\n",
    "            json.dump(original_data, f, indent=2)\n",
    "        print(f\"💾 Backup created: {backup_file}\")\n",
    "    \n",
    "    # Overwrite original file with updated data\n",
    "    with open(input_file, 'w') as f:\n",
    "        json.dump(strategyqa_data, f, indent=2)\n",
    "    print(f\"🔄 Original file updated: {input_file}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ File not found: {input_file}\")\n",
    "    print(\"   Make sure the StrategyQA final dataset file exists in the current directory\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca1ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34ae7353",
   "metadata": {},
   "source": [
    "# 📚 StrategyQA Chain-of-Thought Generation Setup\n",
    "\n",
    "## 🎯 **What We've Created:**\n",
    "\n",
    "### **1. StrategyQA-Specific CoT Function**\n",
    "- **`generate_cot_strategyqa()`** - Designed for yes/no reasoning with external knowledge\n",
    "- **4 Prompt Types Available:**\n",
    "  - `comprehensive` ⭐ **RECOMMENDED** - Includes term, description, facts, question\n",
    "  - `question_focused` - Question + facts with background\n",
    "  - `minimal` - Question + facts only\n",
    "  - `default` - Simple approach\n",
    "\n",
    "### **2. Dataset Structure**\n",
    "- **Source:** `ChilleD/StrategyQA` (1603 samples total)\n",
    "- **Target:** 1500 samples (as requested)\n",
    "- **Output Format:** JSON with fields: `qid`, `term`, `description`, `question`, `facts`, `cot`, `ans`, `gold`, `domain`, `prompt_type`\n",
    "\n",
    "### **3. Generation Configuration**\n",
    "```python\n",
    "STRATEGYQA_TOTAL_SAMPLES = 1500      # As requested by user\n",
    "STRATEGYQA_CHECKPOINT_EVERY = 25     # Save every 25 samples\n",
    "STRATEGYQA_CHECKPOINT_FILE = \"cot_strategyqa_checkpoint.json\"\n",
    "```\n",
    "\n",
    "## 🏆 **Why 'comprehensive' Prompt is Essential for StrategyQA:**\n",
    "\n",
    "**StrategyQA Problems Require External Knowledge:**\n",
    "- **Term**: \"Mixed martial arts\" \n",
    "- **Description**: \"full contact combat sport\"\n",
    "- **Facts**: \"Mixed Martial arts in the UFC takes place in an enclosed structure called The Octagon. The Roman Colosseum was an enclosed structure where gladiators would fight.\"\n",
    "- **Question**: \"Is Mixed martial arts totally original from Roman Colosseum games?\"\n",
    "\n",
    "**Why All Context is Needed:**\n",
    "1. **External Knowledge Dependency** - Unlike math problems, requires background information\n",
    "2. **Complex Reasoning** - Must connect historical facts with modern concepts\n",
    "3. **Yes/No Decision Making** - Needs complete context for accurate boolean answers\n",
    "4. **Fact Integration** - Must synthesize multiple pieces of evidence\n",
    "\n",
    "## 🚀 **Generation Process:**\n",
    "1. **Test First** - Uncomment test lines to verify one sample works\n",
    "2. **Run Generation** - Execute the generation loop for 1500 samples\n",
    "3. **Monitor Progress** - Checkpoints save every 25 samples\n",
    "4. **Estimated Time** - ~100 minutes (1500 × 4 seconds + processing)\n",
    "\n",
    "## 📊 **Expected Output Quality:**\n",
    "- **High-quality reasoning** with step-by-step analysis\n",
    "- **Boolean extraction** (True/False) from CoT text\n",
    "- **Context-aware answers** leveraging all provided information\n",
    "- **Research-ready dataset** for thesis work\n",
    "\n",
    "**Ready to generate 1500 StrategyQA Chain-of-Thought samples!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa85bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LOADING COMMONSENSEQA DATASET...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77902c39bf874e2a87e3ad8009c5d5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/757 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nooba\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nooba\\.cache\\huggingface\\hub\\datasets--ChilleD--CommonSenseQA. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e0e66652a143c1895fad0caed0455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1786d888a6b84408b2c39769d12c32e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/278k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f2e6a0822a49419d84cc92a54c6871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/264k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6e2687c3ab4aa18fdbb86139244db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32888eaa7cfe49f78813545d81555a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3561ad0ace4f74a8c950afac902db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded ChilleD/CommonSenseQA dataset!\n",
      "\n",
      "📊 DATASET OVERVIEW:\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "\n",
      "🔍 TRAIN SPLIT:\n",
      "   • Total samples: 9741\n",
      "   • Sample keys: ['id', 'question', 'question_concept', 'choices', 'answerKey', 'question_concat']\n",
      "\n",
      "🔍 VALIDATION SPLIT:\n",
      "   • Total samples: 1221\n",
      "   • Sample keys: ['id', 'question', 'question_concept', 'choices', 'answerKey', 'question_concat']\n",
      "\n",
      "🔍 TEST SPLIT:\n",
      "   • Total samples: 1140\n",
      "   • Sample keys: ['id', 'question', 'question_concept', 'choices', 'answerKey', 'question_concat']\n",
      "\n",
      "📋 DETAILED STRUCTURE ANALYSIS (TRAIN SPLIT):\n",
      "==================================================\n",
      "🔍 FIRST SAMPLE BREAKDOWN:\n",
      "   id: 075e483d21c29a511267ef62bedc0461\n",
      "   question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the scho...\n",
      "   question_concept: punishing\n",
      "   choices: {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}\n",
      "   answerKey: A\n",
      "   question_concat: The sanctions against the school were a punishing blow, and they seemed to what the efforts the scho...\n",
      "\n",
      "📝 SAMPLE QUESTIONS (First 3):\n",
      "\n",
      "Example 1:\n",
      "   ID: 075e483d21c29a511267ef62bedc0461\n",
      "   Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "   Choices: {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}\n",
      "   Answer Key: A\n",
      "   Concept: punishing\n",
      "\n",
      "Example 2:\n",
      "   ID: 61fe6e879ff18686d7552425a36344c8\n",
      "   Question: Sammy wanted to go to where the people were.  Where might he go?\n",
      "   Choices: {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']}\n",
      "   Answer Key: B\n",
      "   Concept: people\n",
      "\n",
      "Example 3:\n",
      "   ID: 4c1cb0e95b99f72d55c068ba0255c54d\n",
      "   Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
      "   Choices: {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}\n",
      "   Answer Key: A\n",
      "   Concept: choker\n",
      "\n",
      "📊 ANSWER DISTRIBUTION ANALYSIS:\n",
      "❌ Error loading CommonSenseQA dataset: 'str' object has no attribute 'get'\n",
      "   Checking if dataset exists or trying alternative names...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# COMMONSENSEQA DATASET LOADING AND STRUCTURE ANALYSIS\n",
    "print(\"🔍 LOADING COMMONSENSEQA DATASET...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load CommonSenseQA dataset\n",
    "try:\n",
    "    commonsenseqa_ds = load_dataset(\"ChilleD/CommonSenseQA\")\n",
    "    print(f\"✅ Successfully loaded ChilleD/CommonSenseQA dataset!\")\n",
    "    \n",
    "    print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "    print(f\"Available splits: {list(commonsenseqa_ds.keys())}\")\n",
    "    \n",
    "    # Analyze each split\n",
    "    for split_name, split_data in commonsenseqa_ds.items():\n",
    "        print(f\"\\n🔍 {split_name.upper()} SPLIT:\")\n",
    "        print(f\"   • Total samples: {len(split_data)}\")\n",
    "        print(f\"   • Sample keys: {list(split_data[0].keys())}\")\n",
    "    \n",
    "    # Focus on train split for detailed analysis\n",
    "    train_data = commonsenseqa_ds['train']\n",
    "    print(f\"\\n📋 DETAILED STRUCTURE ANALYSIS (TRAIN SPLIT):\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "    # Show first sample with full structure\n",
    "    sample = train_data[0]\n",
    "    print(f\"🔍 FIRST SAMPLE BREAKDOWN:\")\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, str):\n",
    "            preview = value[:100] + \"...\" if len(value) > 100 else value\n",
    "        elif isinstance(value, list):\n",
    "            preview = f\"List with {len(value)} items: {value}\"\n",
    "        else:\n",
    "            preview = str(value)\n",
    "        print(f\"   {key}: {preview}\")\n",
    "    \n",
    "    # Show a few more samples to understand variety\n",
    "    print(f\"\\n📝 SAMPLE QUESTIONS (First 3):\")\n",
    "    for i in range(min(3, len(train_data))):\n",
    "        sample = train_data[i]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"   ID: {sample.get('id', 'N/A')}\")\n",
    "        print(f\"   Question: {sample.get('question', 'N/A')}\")\n",
    "        print(f\"   Choices: {sample.get('choices', 'N/A')}\")\n",
    "        print(f\"   Answer Key: {sample.get('answerKey', 'N/A')}\")\n",
    "        if 'question_concept' in sample:\n",
    "            print(f\"   Concept: {sample.get('question_concept', 'N/A')}\")\n",
    "    \n",
    "    # Analyze answer distribution\n",
    "    print(f\"\\n📊 ANSWER DISTRIBUTION ANALYSIS:\")\n",
    "    if 'answerKey' in train_data[0]:\n",
    "        answer_dist = {}\n",
    "        for sample in train_data[:100]:  # Sample first 100\n",
    "            answer = sample.get('answerKey', 'Unknown')\n",
    "            answer_dist[answer] = answer_dist.get(answer, 0) + 1\n",
    "        print(f\"   Answer distribution (first 100): {answer_dist}\")\n",
    "    \n",
    "    # Analyze choice structure\n",
    "    print(f\"\\n🔤 CHOICE STRUCTURE ANALYSIS:\")\n",
    "    if 'choices' in train_data[0]:\n",
    "        first_choices = train_data[0]['choices']\n",
    "        print(f\"   Choice format: {type(first_choices)}\")\n",
    "        if isinstance(first_choices, dict):\n",
    "            print(f\"   Choice keys: {list(first_choices.keys())}\")\n",
    "            if 'text' in first_choices:\n",
    "                print(f\"   Number of options: {len(first_choices['text'])}\")\n",
    "                print(f\"   Choice labels: {first_choices.get('label', 'No labels')}\")\n",
    "                print(f\"   Sample choices: {first_choices['text'][:3]}...\")\n",
    "        elif isinstance(first_choices, list):\n",
    "            print(f\"   Number of choices: {len(first_choices)}\")\n",
    "            print(f\"   Sample choices: {first_choices}\")\n",
    "    \n",
    "    print(f\"\\n✅ COMMONSENSEQA DATASET LOADED AND ANALYZED!\")\n",
    "    print(f\"   • Dataset: ChilleD/CommonSenseQA\")\n",
    "    print(f\"   • Format: Multiple choice questions with commonsense reasoning\")\n",
    "    print(f\"   • Ready for Chain-of-Thought generation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading CommonSenseQA dataset: {e}\")\n",
    "    print(\"   Checking if dataset exists or trying alternative names...\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd996a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 COMMONSENSEQA CHAIN-OF-THOUGHT PROMPT STRATEGIES\n",
      "======================================================================\n",
      "📋 SAMPLE QUESTION FOR PROMPT COMPARISON:\n",
      "   Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "   Concept: punishing\n",
      "   Choices: A: ignore, B: enforce, C: authoritarian, D: yell at, E: avoid\n",
      "   Correct Answer: A\n",
      "\n",
      "🔍 PROMPT STRUCTURE OPTIONS:\n",
      "==================================================\n",
      "1️⃣ CONCEPT-GUIDED REASONING (⭐ RECOMMENDED)\n",
      "📝 STRUCTURE:\n",
      "   • Highlights the key concept upfront\n",
      "   • Presents choices in clear A-E format\n",
      "   • 5-step systematic reasoning process\n",
      "   • Encourages concept-first analysis\n",
      "\n",
      "2️⃣ QUESTION-FOCUSED ANALYSIS\n",
      "📝 STRUCTURE:\n",
      "   • Direct question-first approach\n",
      "   • General analysis framework\n",
      "   • Less structured than concept-guided\n",
      "\n",
      "3️⃣ ELIMINATION STRATEGY\n",
      "📝 STRUCTURE:\n",
      "   • Process of elimination approach\n",
      "   • Good for complex multiple choice\n",
      "   • May miss positive reasoning\n",
      "\n",
      "4️⃣ CONTEXTUAL UNDERSTANDING\n",
      "📝 STRUCTURE:\n",
      "   • Deep contextual analysis\n",
      "   • Individual choice examination\n",
      "   • Relationship-focused reasoning\n",
      "\n",
      "🏆 RECOMMENDATION ANALYSIS:\n",
      "==================================================\n",
      "⭐ **RECOMMENDED: CONCEPT-GUIDED REASONING** ⭐\n",
      "\n",
      "🎯 **Why this approach works best for CommonSenseQA:**\n",
      "   1. **Concept-First Design**: CommonSenseQA is built around key concepts\n",
      "   2. **Systematic Process**: 5-step structure ensures thorough reasoning\n",
      "   3. **Commonsense Focus**: Explicitly calls for commonsense application\n",
      "   4. **Clear Choice Format**: A-E presentation matches dataset structure\n",
      "   5. **Proven Effective**: Similar to successful CoT approaches in literature\n",
      "\n",
      "📊 **Comparative Advantages:**\n",
      "   • More structured than Question-Focused\n",
      "   • More positive reasoning than Elimination\n",
      "   • More systematic than Contextual Understanding\n",
      "   • Leverages the concept field that makes CommonSenseQA unique\n",
      "\n",
      "⚙️ **Expected Benefits:**\n",
      "   • Higher accuracy due to concept grounding\n",
      "   • More consistent reasoning patterns\n",
      "   • Better extraction reliability (clear A-E structure)\n",
      "   • Robust performance across different question types\n",
      "\n",
      "🧪 **Alternative Uses:**\n",
      "   • **Complex questions**: Use Elimination Strategy\n",
      "   • **Ambiguous context**: Use Contextual Understanding\n",
      "   • **Simple questions**: Use Question-Focused Analysis\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# COMMONSENSEQA PROMPT STRUCTURE RECOMMENDATIONS\n",
    "print(\"🎯 COMMONSENSEQA CHAIN-OF-THOUGHT PROMPT STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def demonstrate_commonsenseqa_prompts():\n",
    "    \"\"\"\n",
    "    Demonstrate different prompt structures for CommonSenseQA CoT generation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample question for demonstration\n",
    "    sample_question = {\n",
    "        'id': '075e483d21c29a511267ef62bedc0461',\n",
    "        'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?',\n",
    "        'question_concept': 'punishing',\n",
    "        'choices': {\n",
    "            'label': ['A', 'B', 'C', 'D', 'E'], \n",
    "            'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']\n",
    "        },\n",
    "        'answerKey': 'A'\n",
    "    }\n",
    "    \n",
    "    print(\"📋 SAMPLE QUESTION FOR PROMPT COMPARISON:\")\n",
    "    print(f\"   Question: {sample_question['question']}\")\n",
    "    print(f\"   Concept: {sample_question['question_concept']}\")\n",
    "    choices_str = \", \".join([f\"{label}: {text}\" for label, text in zip(sample_question['choices']['label'], sample_question['choices']['text'])])\n",
    "    print(f\"   Choices: {choices_str}\")\n",
    "    print(f\"   Correct Answer: {sample_question['answerKey']}\")\n",
    "    \n",
    "    print(f\"\\n🔍 PROMPT STRUCTURE OPTIONS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # OPTION 1: Concept-Guided Reasoning (RECOMMENDED)\n",
    "    print(\"1️⃣ CONCEPT-GUIDED REASONING (⭐ RECOMMENDED)\")\n",
    "    concept_prompt = f\"\"\"I need to solve this commonsense reasoning question by understanding the key concept and analyzing each choice.\n",
    "\n",
    "**Key Concept:** {sample_question['question_concept']}\n",
    "\n",
    "**Question:** {sample_question['question']}\n",
    "\n",
    "**Available Choices:**\n",
    "A) {sample_question['choices']['text'][0]}\n",
    "B) {sample_question['choices']['text'][1]}\n",
    "C) {sample_question['choices']['text'][2]}\n",
    "D) {sample_question['choices']['text'][3]}\n",
    "E) {sample_question['choices']['text'][4]}\n",
    "\n",
    "**Step-by-step reasoning:**\n",
    "1. **Understand the concept**: What does \"{sample_question['question_concept']}\" mean in this context?\n",
    "2. **Analyze the situation**: What is the question really asking?\n",
    "3. **Evaluate each choice**: How does each option relate to the concept and situation?\n",
    "4. **Apply commonsense**: Which choice makes the most logical sense?\n",
    "5. **Final answer**: Select the best choice with reasoning.\n",
    "\n",
    "Let me work through this systematically:\"\"\"\n",
    "    \n",
    "    print(\"📝 STRUCTURE:\")\n",
    "    print(\"   • Highlights the key concept upfront\")\n",
    "    print(\"   • Presents choices in clear A-E format\")\n",
    "    print(\"   • 5-step systematic reasoning process\")\n",
    "    print(\"   • Encourages concept-first analysis\")\n",
    "    \n",
    "    # OPTION 2: Question-Focused Analysis\n",
    "    print(f\"\\n2️⃣ QUESTION-FOCUSED ANALYSIS\")\n",
    "    question_prompt = f\"\"\"Please solve this commonsense reasoning question step by step.\n",
    "\n",
    "**Question:** {sample_question['question']}\n",
    "\n",
    "**Choices:**\n",
    "A) {sample_question['choices']['text'][0]}\n",
    "B) {sample_question['choices']['text'][1]}\n",
    "C) {sample_question['choices']['text'][2]}\n",
    "D) {sample_question['choices']['text'][3]}\n",
    "E) {sample_question['choices']['text'][4]}\n",
    "\n",
    "**Analysis approach:**\n",
    "- What is the main situation described?\n",
    "- What relationship or outcome is being asked about?\n",
    "- Which choice best fits the logical flow?\n",
    "- What does common sense tell us?\n",
    "\n",
    "**Step-by-step reasoning:**\"\"\"\n",
    "    \n",
    "    print(\"📝 STRUCTURE:\")\n",
    "    print(\"   • Direct question-first approach\")\n",
    "    print(\"   • General analysis framework\")\n",
    "    print(\"   • Less structured than concept-guided\")\n",
    "    \n",
    "    # OPTION 3: Elimination Strategy\n",
    "    print(f\"\\n3️⃣ ELIMINATION STRATEGY\")\n",
    "    elimination_prompt = f\"\"\"Let me solve this by systematically eliminating incorrect choices.\n",
    "\n",
    "**Question:** {sample_question['question']}\n",
    "\n",
    "**All Choices:**\n",
    "A) {sample_question['choices']['text'][0]}\n",
    "B) {sample_question['choices']['text'][1]}\n",
    "C) {sample_question['choices']['text'][2]}\n",
    "D) {sample_question['choices']['text'][3]}\n",
    "E) {sample_question['choices']['text'][4]}\n",
    "\n",
    "**Elimination process:**\n",
    "1. **Identify clearly wrong choices**: Which options don't make sense at all?\n",
    "2. **Remove unlikely options**: Which choices are possible but improbable?\n",
    "3. **Compare remaining choices**: Among the viable options, which is best?\n",
    "4. **Final verification**: Does my choice answer the question logically?\n",
    "\n",
    "**Reasoning:**\"\"\"\n",
    "    \n",
    "    print(\"📝 STRUCTURE:\")\n",
    "    print(\"   • Process of elimination approach\")\n",
    "    print(\"   • Good for complex multiple choice\")\n",
    "    print(\"   • May miss positive reasoning\")\n",
    "    \n",
    "    # OPTION 4: Contextual Understanding\n",
    "    print(f\"\\n4️⃣ CONTEXTUAL UNDERSTANDING\")\n",
    "    context_prompt = f\"\"\"I need to understand the full context and relationships in this question.\n",
    "\n",
    "**Context Analysis:**\n",
    "- Situation: {sample_question['question'].split(',')[0]}\n",
    "- Key relationship: What effect is being described?\n",
    "- Concept focus: \"{sample_question['question_concept']}\"\n",
    "\n",
    "**Question:** {sample_question['question']}\n",
    "\n",
    "**Options to consider:**\n",
    "A) {sample_question['choices']['text'][0]} - How does this relate to the situation?\n",
    "B) {sample_question['choices']['text'][1]} - What would this mean in context?\n",
    "C) {sample_question['choices']['text'][2]} - Does this fit the relationship?\n",
    "D) {sample_question['choices']['text'][3]} - Is this a logical outcome?\n",
    "E) {sample_question['choices']['text'][4]} - How does this connect to the scenario?\n",
    "\n",
    "**Reasoning through context:**\"\"\"\n",
    "    \n",
    "    print(\"📝 STRUCTURE:\")\n",
    "    print(\"   • Deep contextual analysis\")\n",
    "    print(\"   • Individual choice examination\")\n",
    "    print(\"   • Relationship-focused reasoning\")\n",
    "    \n",
    "    print(f\"\\n🏆 RECOMMENDATION ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"⭐ **RECOMMENDED: CONCEPT-GUIDED REASONING** ⭐\")\n",
    "    print()\n",
    "    print(\"🎯 **Why this approach works best for CommonSenseQA:**\")\n",
    "    print(\"   1. **Concept-First Design**: CommonSenseQA is built around key concepts\")\n",
    "    print(\"   2. **Systematic Process**: 5-step structure ensures thorough reasoning\")\n",
    "    print(\"   3. **Commonsense Focus**: Explicitly calls for commonsense application\")\n",
    "    print(\"   4. **Clear Choice Format**: A-E presentation matches dataset structure\")\n",
    "    print(\"   5. **Proven Effective**: Similar to successful CoT approaches in literature\")\n",
    "    \n",
    "    print(f\"\\n📊 **Comparative Advantages:**\")\n",
    "    print(\"   • More structured than Question-Focused\")\n",
    "    print(\"   • More positive reasoning than Elimination\")\n",
    "    print(\"   • More systematic than Contextual Understanding\")\n",
    "    print(\"   • Leverages the concept field that makes CommonSenseQA unique\")\n",
    "    \n",
    "    print(f\"\\n⚙️ **Expected Benefits:**\")\n",
    "    print(\"   • Higher accuracy due to concept grounding\")\n",
    "    print(\"   • More consistent reasoning patterns\")\n",
    "    print(\"   • Better extraction reliability (clear A-E structure)\")\n",
    "    print(\"   • Robust performance across different question types\")\n",
    "    \n",
    "    print(f\"\\n🧪 **Alternative Uses:**\")\n",
    "    print(\"   • **Complex questions**: Use Elimination Strategy\")\n",
    "    print(\"   • **Ambiguous context**: Use Contextual Understanding\")\n",
    "    print(\"   • **Simple questions**: Use Question-Focused Analysis\")\n",
    "    \n",
    "    return concept_prompt\n",
    "\n",
    "# Demonstrate the prompts\n",
    "recommended_prompt = demonstrate_commonsenseqa_prompts()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f55fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COMMONSENSEQA CoT generation function created!\n",
      "\n",
      "📋 AVAILABLE PROMPT TYPES:\n",
      "1. 'concept_guided' (RECOMMENDED) - Systematic concept-based reasoning\n",
      "2. 'question_focused' - Direct question analysis approach\n",
      "3. 'elimination' - Process of elimination strategy\n",
      "4. 'default' - Simple approach\n",
      "\n",
      "🎯 RECOMMENDED CHOICE: 'concept_guided'\n",
      "   REASON: Leverages CommonSenseQA's concept field for better reasoning\n",
      "   FEATURES:\n",
      "   • 5-step systematic process\n",
      "   • Concept-first analysis\n",
      "   • Clear A-E choice presentation\n",
      "   • Commonsense application focus\n"
     ]
    }
   ],
   "source": [
    "# COMMONSENSEQA CoT GENERATION FUNCTION\n",
    "def generate_cot_commonsenseqa(sample, prompt_type=\"concept_guided\"):\n",
    "    \"\"\"\n",
    "    Enhanced CoT generation specifically designed for CommonSenseQA dataset\n",
    "    \n",
    "    CommonSenseQA has multiple choice questions (A-E) with:\n",
    "    - question: The main question text\n",
    "    - question_concept: Key concept/theme\n",
    "    - choices: Dictionary with 'label' and 'text' arrays\n",
    "    - answerKey: Correct answer (A, B, C, D, or E)\n",
    "    \"\"\"\n",
    "    \n",
    "    if prompt_type == \"concept_guided\":\n",
    "        # RECOMMENDED: Concept-guided systematic reasoning\n",
    "        choices_text = \"\\n\".join([f\"{label}) {text}\" for label, text in zip(sample['choices']['label'], sample['choices']['text'])])\n",
    "        \n",
    "        prompt = f\"\"\"I need to solve this commonsense reasoning question by understanding the key concept and analyzing each choice.\n",
    "\n",
    "**Key Concept:** {sample['question_concept']}\n",
    "\n",
    "**Question:** {sample['question']}\n",
    "\n",
    "**Available Choices:**\n",
    "{choices_text}\n",
    "\n",
    "**Step-by-step reasoning:**\n",
    "1. **Understand the concept**: What does \"{sample['question_concept']}\" mean in this context?\n",
    "2. **Analyze the situation**: What is the question really asking?\n",
    "3. **Evaluate each choice**: How does each option relate to the concept and situation?\n",
    "4. **Apply commonsense**: Which choice makes the most logical sense?\n",
    "5. **Final answer**: Select the best choice with reasoning.\n",
    "\n",
    "Let me work through this systematically:\"\"\"\n",
    "\n",
    "    elif prompt_type == \"question_focused\":\n",
    "        # Alternative: Direct question analysis\n",
    "        choices_text = \"\\n\".join([f\"{label}) {text}\" for label, text in zip(sample['choices']['label'], sample['choices']['text'])])\n",
    "        \n",
    "        prompt = f\"\"\"Please solve this commonsense reasoning question step by step.\n",
    "\n",
    "**Question:** {sample['question']}\n",
    "\n",
    "**Choices:**\n",
    "{choices_text}\n",
    "\n",
    "**Analysis approach:**\n",
    "- What is the main situation described?\n",
    "- What relationship or outcome is being asked about?\n",
    "- Which choice best fits the logical flow?\n",
    "- What does common sense tell us?\n",
    "\n",
    "**Step-by-step reasoning:\"\"\"\n",
    "\n",
    "    elif prompt_type == \"elimination\":\n",
    "        # Alternative: Elimination strategy\n",
    "        choices_text = \"\\n\".join([f\"{label}) {text}\" for label, text in zip(sample['choices']['label'], sample['choices']['text'])])\n",
    "        \n",
    "        prompt = f\"\"\"Let me solve this by systematically eliminating incorrect choices.\n",
    "\n",
    "**Question:** {sample['question']}\n",
    "\n",
    "**All Choices:**\n",
    "{choices_text}\n",
    "\n",
    "**Elimination process:**\n",
    "1. **Identify clearly wrong choices**: Which options don't make sense at all?\n",
    "2. **Remove unlikely options**: Which choices are possible but improbable?\n",
    "3. **Compare remaining choices**: Among the viable options, which is best?\n",
    "4. **Final verification**: Does my choice answer the question logically?\n",
    "\n",
    "**Reasoning:\"\"\"\n",
    "\n",
    "    else:  # default - simple approach\n",
    "        choices_text = \", \".join([f\"{label}: {text}\" for label, text in zip(sample['choices']['label'], sample['choices']['text'])])\n",
    "        prompt = f\"Question: {sample['question']}\\nChoices: {choices_text}\\n\\nLet me think step by step and choose the best answer.\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        trace = response.text\n",
    "        \n",
    "        # Extract A-E answer from CoT\n",
    "        # CommonSenseQA answers are single letters (A, B, C, D, E)\n",
    "        \n",
    "        # Method 1: Look for explicit answer patterns\n",
    "        answer_patterns = [\n",
    "            r'(?:final\\s+)?(?:answer|choice)[\\s:]*(?:is\\s+)?(?:\\*\\*)?([A-E])(?:\\*\\*)?',\n",
    "            r'(?:the\\s+)?answer[\\s:]+(?:\\*\\*)?([A-E])(?:\\*\\*)?',\n",
    "            r'(?:\\*\\*)?([A-E])(?:\\*\\*)?(?:\\s*[\\.!])?$',  # A-E at end of line\n",
    "            r'(?:therefore|so|thus)[\\s,]*(?:the\\s+answer\\s+is\\s+)?(?:\\*\\*)?([A-E])(?:\\*\\*)?',\n",
    "            r'(?:choose|select)[\\s,]*(?:option\\s+)?(?:\\*\\*)?([A-E])(?:\\*\\*)?',\n",
    "            r'(?:option|choice)\\s+(?:\\*\\*)?([A-E])(?:\\*\\*)?',\n",
    "        ]\n",
    "        \n",
    "        extracted_answer = None\n",
    "        for pattern in answer_patterns:\n",
    "            matches = re.findall(pattern, trace, re.IGNORECASE)\n",
    "            if matches:\n",
    "                # Get the last match (most likely final answer)\n",
    "                extracted_answer = matches[-1].upper()\n",
    "                break\n",
    "        \n",
    "        # Method 2: Fallback - look for A-E in final lines\n",
    "        if not extracted_answer:\n",
    "            lines = trace.split('\\n')\n",
    "            for line in reversed(lines[-5:]):  # Check last 5 lines\n",
    "                # Look for isolated A, B, C, D, E\n",
    "                isolated_letters = re.findall(r'\\b([A-E])\\b', line.upper())\n",
    "                if isolated_letters:\n",
    "                    extracted_answer = isolated_letters[-1]  # Take the last one\n",
    "                    break\n",
    "        \n",
    "        # Method 3: Final fallback - look for \"A)\" style patterns\n",
    "        if not extracted_answer:\n",
    "            answer_with_paren = re.findall(r'([A-E])\\)', trace, re.IGNORECASE)\n",
    "            if answer_with_paren:\n",
    "                extracted_answer = answer_with_paren[-1].upper()\n",
    "        \n",
    "        return {\n",
    "            \"id\": sample['id'],\n",
    "            \"question\": sample['question'],\n",
    "            \"question_concept\": sample['question_concept'],\n",
    "            \"choices\": sample['choices'],\n",
    "            \"cot\": trace,\n",
    "            \"ans\": extracted_answer if extracted_answer else sample['answerKey'],  # Fallback to gold if extraction fails\n",
    "            \"gold\": sample['answerKey'],\n",
    "            \"domain\": \"commonsenseqa\",\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content for CommonSenseQA problem: {sample['id']}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return {\n",
    "            \"id\": sample['id'],\n",
    "            \"question\": sample['question'],\n",
    "            \"question_concept\": sample['question_concept'],\n",
    "            \"choices\": sample['choices'],\n",
    "            \"cot\": f\"Error: {e}\",\n",
    "            \"ans\": sample['answerKey'],\n",
    "            \"gold\": sample['answerKey'],\n",
    "            \"domain\": \"commonsenseqa\",\n",
    "            \"prompt_type\": prompt_type\n",
    "        }\n",
    "\n",
    "print(\"✅ COMMONSENSEQA CoT generation function created!\")\n",
    "print(\"\\n📋 AVAILABLE PROMPT TYPES:\")\n",
    "print(\"1. 'concept_guided' (RECOMMENDED) - Systematic concept-based reasoning\")\n",
    "print(\"2. 'question_focused' - Direct question analysis approach\")\n",
    "print(\"3. 'elimination' - Process of elimination strategy\")\n",
    "print(\"4. 'default' - Simple approach\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED CHOICE: 'concept_guided'\")\n",
    "print(\"   REASON: Leverages CommonSenseQA's concept field for better reasoning\")\n",
    "print(\"   FEATURES:\")\n",
    "print(\"   • 5-step systematic process\")\n",
    "print(\"   • Concept-first analysis\")\n",
    "print(\"   • Clear A-E choice presentation\")\n",
    "print(\"   • Commonsense application focus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d82f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 PREPARING COMMONSENSEQA DATASET FOR COT GENERATION...\n",
      "📊 CommonSenseQA Dataset prepared: 9741 problems\n",
      "   Sample concepts: ['seafood restaurant', 'lying', 'distance', 'bench', 'choker', 'sun', 'skiing', 'fountain pen', 'run errands', 'watching tv']...\n",
      "📄 No checkpoint found - starting from beginning\n",
      "\n",
      "🚀 COMMONSENSEQA COT GENERATION STATUS:\n",
      "   • Total target: 1000\n",
      "   • Already completed: 0\n",
      "   • Remaining: 1000\n",
      "   • Checkpoint every: 25 samples\n",
      "   • Recommended prompt: 'concept_guided'\n",
      "   • Rate limit: 4 seconds between requests\n",
      "   • Estimated time: ~66 minutes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# COMMONSENSEQA DATASET PREPARATION AND CONFIGURATION\n",
    "print(\"🔄 PREPARING COMMONSENSEQA DATASET FOR COT GENERATION...\")\n",
    "\n",
    "# Prepare CommonSenseQA questions (use train split)\n",
    "commonsenseqa_train = commonsenseqa_ds['train']\n",
    "commonsenseqa_questions = []\n",
    "\n",
    "for sample in commonsenseqa_train:\n",
    "    commonsenseqa_questions.append({\n",
    "        'id': sample['id'],\n",
    "        'question': sample['question'],\n",
    "        'question_concept': sample['question_concept'],\n",
    "        'choices': sample['choices'],\n",
    "        'answerKey': sample['answerKey']\n",
    "    })\n",
    "\n",
    "print(f\"📊 CommonSenseQA Dataset prepared: {len(commonsenseqa_questions)} problems\")\n",
    "\n",
    "# Show sample of different concepts\n",
    "concepts = [q['question_concept'] for q in commonsenseqa_questions[:50]]\n",
    "unique_concepts = list(set(concepts))\n",
    "print(f\"   Sample concepts: {unique_concepts[:10]}...\")\n",
    "\n",
    "# COMMONSENSEQA GENERATION CONFIGURATION\n",
    "COMMONSENSEQA_TOTAL_SAMPLES = 1000  # USER REQUESTED: 1000 samples\n",
    "COMMONSENSEQA_CHECKPOINT_EVERY = 25  # Save progress every N samples\n",
    "COMMONSENSEQA_CHECKPOINT_FILE = \"cot_commonsenseqa_checkpoint.json\"\n",
    "\n",
    "# Load existing progress if any\n",
    "commonsenseqa_dataset = []\n",
    "commonsenseqa_start_index = 0\n",
    "\n",
    "if os.path.exists(COMMONSENSEQA_CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(COMMONSENSEQA_CHECKPOINT_FILE, 'r') as f:\n",
    "            commonsenseqa_dataset = json.load(f)\n",
    "        commonsenseqa_start_index = len(commonsenseqa_dataset)\n",
    "        print(f\"📁 FOUND CHECKPOINT: {commonsenseqa_start_index} samples already completed\")\n",
    "        print(f\"📊 Progress: {commonsenseqa_start_index}/{COMMONSENSEQA_TOTAL_SAMPLES} ({commonsenseqa_start_index/COMMONSENSEQA_TOTAL_SAMPLES*100:.1f}%)\")\n",
    "        \n",
    "        if commonsenseqa_start_index > 0:\n",
    "            print(f\"✅ Resuming from sample #{commonsenseqa_start_index + 1}\")\n",
    "            print(f\"🚀 Remaining: {COMMONSENSEQA_TOTAL_SAMPLES - commonsenseqa_start_index} samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CommonSenseQA checkpoint: {e}\")\n",
    "        print(\"🔄 Starting fresh...\")\n",
    "        commonsenseqa_dataset = []\n",
    "        commonsenseqa_start_index = 0\n",
    "else:\n",
    "    print(\"📄 No checkpoint found - starting from beginning\")\n",
    "\n",
    "print(f\"\\n🚀 COMMONSENSEQA COT GENERATION STATUS:\")\n",
    "print(f\"   • Total target: {COMMONSENSEQA_TOTAL_SAMPLES}\")\n",
    "print(f\"   • Already completed: {commonsenseqa_start_index}\")\n",
    "print(f\"   • Remaining: {COMMONSENSEQA_TOTAL_SAMPLES - commonsenseqa_start_index}\")\n",
    "print(f\"   • Checkpoint every: {COMMONSENSEQA_CHECKPOINT_EVERY} samples\")\n",
    "print(f\"   • Recommended prompt: 'concept_guided'\")\n",
    "print(f\"   • Rate limit: 4 seconds between requests\")\n",
    "print(f\"   • Estimated time: ~{((COMMONSENSEQA_TOTAL_SAMPLES - commonsenseqa_start_index) * 4) // 60} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "107d7be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating 1000 remaining CommonSenseQA CoT samples...\n",
      "📍 Starting from sample #1\n",
      "💾 Checkpoints every 25 samples\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:   2%|▏         | 24/1000 [03:08<1:58:40,  7.30s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 25/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:   5%|▍         | 49/1000 [06:22<1:49:43,  6.92s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 50/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:   7%|▋         | 74/1000 [09:22<1:49:52,  7.12s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 75/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  10%|▉         | 99/1000 [12:29<1:40:42,  6.71s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 100/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  12%|█▏        | 124/1000 [15:30<2:06:18,  8.65s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 125/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  15%|█▍        | 149/1000 [18:34<1:35:22,  6.72s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 150/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  17%|█▋        | 174/1000 [21:41<1:39:59,  7.26s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 175/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  20%|█▉        | 199/1000 [24:43<1:37:47,  7.33s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 200/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  22%|██▏       | 224/1000 [27:55<2:13:43, 10.34s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 225/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  25%|██▍       | 249/1000 [31:03<1:45:51,  8.46s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 250/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  27%|██▋       | 274/1000 [34:01<1:23:59,  6.94s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 275/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  30%|██▉       | 299/1000 [37:02<1:23:04,  7.11s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 300/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  32%|███▏      | 324/1000 [40:19<1:20:28,  7.14s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 325/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  35%|███▍      | 349/1000 [43:24<1:17:38,  7.16s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 350/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  37%|███▋      | 374/1000 [46:34<1:11:56,  6.90s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 375/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  40%|███▉      | 399/1000 [49:30<1:11:44,  7.16s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 400/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  42%|████▏     | 424/1000 [52:30<1:06:36,  6.94s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 425/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  45%|████▍     | 449/1000 [55:37<1:11:31,  7.79s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 450/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  47%|████▋     | 474/1000 [58:41<1:03:13,  7.21s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 475/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  50%|████▉     | 499/1000 [1:02:09<1:00:04,  7.19s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 500/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  52%|█████▏    | 524/1000 [1:05:12<58:46,  7.41s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 525/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  55%|█████▍    | 549/1000 [1:08:18<53:42,  7.15s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 550/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  57%|█████▋    | 574/1000 [1:11:24<50:02,  7.05s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 575/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  60%|█████▉    | 599/1000 [1:14:24<44:55,  6.72s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 600/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  62%|██████▏   | 624/1000 [1:17:41<48:34,  7.75s/problem]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 625/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  65%|██████▍   | 649/1000 [1:20:54<41:56,  7.17s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 650/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  67%|██████▋   | 674/1000 [1:23:47<37:09,  6.84s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 675/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  70%|██████▉   | 699/1000 [1:26:54<39:34,  7.89s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 700/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  72%|███████▏  | 724/1000 [1:29:55<32:35,  7.09s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 725/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  75%|███████▍  | 749/1000 [1:32:47<27:57,  6.68s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 750/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  77%|███████▋  | 774/1000 [1:35:43<25:58,  6.89s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 775/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  80%|███████▉  | 799/1000 [1:38:53<23:55,  7.14s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 800/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  82%|████████▏ | 824/1000 [1:41:46<20:48,  7.10s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 825/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  85%|████████▍ | 849/1000 [1:44:53<17:48,  7.08s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 850/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  87%|████████▋ | 874/1000 [1:47:53<14:04,  6.70s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 875/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  90%|████████▉ | 899/1000 [1:51:03<13:04,  7.77s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 900/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  92%|█████████▏| 924/1000 [1:54:04<10:14,  8.08s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 925/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  95%|█████████▍| 949/1000 [1:56:58<05:34,  6.56s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 950/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000:  97%|█████████▋| 974/1000 [2:00:02<03:34,  8.25s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 975/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000: 100%|█████████▉| 999/1000 [2:03:23<00:07,  7.19s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 CommonSenseQA Checkpoint: 1000/1000 samples saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonSenseQA 1-1000: 100%|██████████| 1000/1000 [2:03:33<00:00,  7.41s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CommonSenseQA Generation complete! 1000 samples saved to cot_commonsenseqa_checkpoint.json\n",
      "🏁 Final CommonSenseQA dataset: cot_commonsenseqa_final_1000.json\n",
      "\n",
      "📋 SAMPLE GENERATED COMMONSENSEQA COT:\n",
      "   ID: 075e483d21c29a511267ef62bedc0461\n",
      "   Concept: punishing\n",
      "   Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the scho...\n",
      "   Generated Answer: E\n",
      "   Gold Answer: A\n",
      "   CoT Length: 3237 characters\n",
      "   📊 Final Accuracy: 64.7% (647/1000)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMONSENSEQA COT GENERATION LOOP\n",
    "if commonsenseqa_start_index >= COMMONSENSEQA_TOTAL_SAMPLES:\n",
    "    print(\"🎉 CommonSenseQA CoT generation already completed! All samples generated.\")\n",
    "    \n",
    "    # Create final file if it doesn't exist\n",
    "    final_file = f\"cot_commonsenseqa_final_{len(commonsenseqa_dataset)}.json\"\n",
    "    if not os.path.exists(final_file):\n",
    "        with open(final_file, \"w\") as f:\n",
    "            json.dump(commonsenseqa_dataset, f, indent=2)\n",
    "        print(f\"🏁 Final dataset created: {final_file}\")\n",
    "    else:\n",
    "        print(f\"📁 Final file already exists: {final_file}\")\n",
    "else:\n",
    "    remaining_samples = COMMONSENSEQA_TOTAL_SAMPLES - commonsenseqa_start_index\n",
    "    print(f\"🚀 Generating {remaining_samples} remaining CommonSenseQA CoT samples...\")\n",
    "    print(f\"📍 Starting from sample #{commonsenseqa_start_index + 1}\")\n",
    "    print(f\"💾 Checkpoints every {COMMONSENSEQA_CHECKPOINT_EVERY} samples\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Generate remaining samples\n",
    "    for i, sample in enumerate(tqdm(commonsenseqa_questions[commonsenseqa_start_index:commonsenseqa_start_index + remaining_samples], \n",
    "                                  desc=f\"Processing CommonSenseQA {commonsenseqa_start_index+1}-{COMMONSENSEQA_TOTAL_SAMPLES}\", \n",
    "                                  unit=\"problem\")):\n",
    "        \n",
    "        # Use concept_guided prompt for best reasoning with systematic approach\n",
    "        entry = generate_cot_commonsenseqa(sample, \"concept_guided\")\n",
    "        commonsenseqa_dataset.append(entry)\n",
    "        \n",
    "        # Checkpoint saving with Windows-compatible atomic write\n",
    "        if len(commonsenseqa_dataset) % COMMONSENSEQA_CHECKPOINT_EVERY == 0:\n",
    "            temp_file = f\"temp_{COMMONSENSEQA_CHECKPOINT_FILE}\"\n",
    "            with open(temp_file, \"w\") as f:\n",
    "                json.dump(commonsenseqa_dataset, f, indent=2)\n",
    "            \n",
    "            # Windows-compatible atomic write\n",
    "            if os.path.exists(COMMONSENSEQA_CHECKPOINT_FILE):\n",
    "                os.remove(COMMONSENSEQA_CHECKPOINT_FILE)\n",
    "            os.rename(temp_file, COMMONSENSEQA_CHECKPOINT_FILE)\n",
    "            print(f\"💾 CommonSenseQA Checkpoint: {len(commonsenseqa_dataset)}/{COMMONSENSEQA_TOTAL_SAMPLES} samples saved\")\n",
    "        \n",
    "        time.sleep(4)  # Rate limit delay\n",
    "\n",
    "    # Final save\n",
    "    with open(COMMONSENSEQA_CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(commonsenseqa_dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ CommonSenseQA Generation complete! {len(commonsenseqa_dataset)} samples saved to {COMMONSENSEQA_CHECKPOINT_FILE}\")\n",
    "\n",
    "    # Create final numbered file\n",
    "    commonsenseqa_final_file = f\"cot_commonsenseqa_final_{len(commonsenseqa_dataset)}.json\"\n",
    "    with open(commonsenseqa_final_file, \"w\") as f:\n",
    "        json.dump(commonsenseqa_dataset, f, indent=2)\n",
    "    print(f\"🏁 Final CommonSenseQA dataset: {commonsenseqa_final_file}\")\n",
    "    \n",
    "    # Show sample of generated data and accuracy\n",
    "    if commonsenseqa_dataset:\n",
    "        print(f\"\\n📋 SAMPLE GENERATED COMMONSENSEQA COT:\")\n",
    "        sample_entry = commonsenseqa_dataset[0]\n",
    "        print(f\"   ID: {sample_entry['id']}\")\n",
    "        print(f\"   Concept: {sample_entry['question_concept']}\")\n",
    "        print(f\"   Question: {sample_entry['question'][:100]}...\")\n",
    "        print(f\"   Generated Answer: {sample_entry['ans']}\")\n",
    "        print(f\"   Gold Answer: {sample_entry['gold']}\")\n",
    "        print(f\"   CoT Length: {len(sample_entry['cot'])} characters\")\n",
    "        \n",
    "        # Check accuracy on all samples\n",
    "        correct = sum(1 for entry in commonsenseqa_dataset if entry['ans'] == entry['gold'])\n",
    "        accuracy = correct / len(commonsenseqa_dataset) * 100\n",
    "        print(f\"   📊 Final Accuracy: {accuracy:.1f}% ({correct}/{len(commonsenseqa_dataset)})\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae501dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 ADDING 'is_correct' COLUMN TO COMMONSENSEQA DATASET\n",
      "============================================================\n",
      "📊 Loaded 1000 samples from cot_commonsenseqa_final_1000.json\n",
      "✅ Successfully added 'is_correct' column!\n",
      "📈 Dataset Statistics:\n",
      "   • Total samples: 1000\n",
      "   • Correct answers: 647\n",
      "   • Incorrect answers: 353\n",
      "   • Accuracy: 64.70%\n",
      "📁 Updated dataset saved as: cot_commonsenseqa_final_1000_with_correct.json\n",
      "\n",
      "📋 SAMPLE ENTRIES WITH 'is_correct' COLUMN:\n",
      "\n",
      "Sample 1:\n",
      "   ID: 075e483d21c29a511267ef62bedc0461\n",
      "   Concept: punishing\n",
      "   Question: The sanctions against the school were a punishing blow, and they seemed to what ...\n",
      "   Generated Answer: E\n",
      "   Gold Answer: A\n",
      "   Is Correct: False\n",
      "\n",
      "Sample 2:\n",
      "   ID: 61fe6e879ff18686d7552425a36344c8\n",
      "   Concept: people\n",
      "   Question: Sammy wanted to go to where the people were.  Where might he go?...\n",
      "   Generated Answer: B\n",
      "   Gold Answer: B\n",
      "   Is Correct: True\n",
      "\n",
      "Sample 3:\n",
      "   ID: 4c1cb0e95b99f72d55c068ba0255c54d\n",
      "   Concept: choker\n",
      "   Question: To locate a choker not located in a jewelry box or boutique where would you go?...\n",
      "   Generated Answer: B\n",
      "   Gold Answer: A\n",
      "   Is Correct: False\n",
      "💾 Backup created: cot_commonsenseqa_final_1000_backup.json\n",
      "🔄 Original file updated: cot_commonsenseqa_final_1000.json\n",
      "\n",
      "📊 ANSWER DISTRIBUTION ANALYSIS:\n",
      "   Generated answers: {'E': 161, 'B': 190, 'D': 158, 'A': 209, 'C': 282}\n",
      "\n",
      "🎯 CONCEPT ACCURACY ANALYSIS (First 100 samples):\n",
      "   punishing: 0.0% (0/1)\n",
      "   people: 100.0% (2/2)\n",
      "   choker: 0.0% (0/1)\n",
      "   highway: 100.0% (1/1)\n",
      "   fox: 0.0% (0/2)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ADD 'is_correct' COLUMN TO COMMONSENSEQA DATASET\n",
    "print(\"🔧 ADDING 'is_correct' COLUMN TO COMMONSENSEQA DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_file = \"cot_commonsenseqa_final_1000.json\"\n",
    "output_file = \"cot_commonsenseqa_final_1000_with_correct.json\"\n",
    "\n",
    "if os.path.exists(input_file):\n",
    "    # Load the dataset\n",
    "    with open(input_file, 'r') as f:\n",
    "        commonsenseqa_data = json.load(f)\n",
    "    \n",
    "    print(f\"📊 Loaded {len(commonsenseqa_data)} samples from {input_file}\")\n",
    "    \n",
    "    # Add 'is_correct' column\n",
    "    correct_count = 0\n",
    "    for entry in commonsenseqa_data:\n",
    "        # Compare generated answer with gold answer\n",
    "        is_correct = entry['ans'] == entry['gold']\n",
    "        entry['is_correct'] = is_correct\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(commonsenseqa_data, f, indent=2)\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    total_samples = len(commonsenseqa_data)\n",
    "    accuracy = (correct_count / total_samples) * 100\n",
    "    \n",
    "    print(f\"✅ Successfully added 'is_correct' column!\")\n",
    "    print(f\"📈 Dataset Statistics:\")\n",
    "    print(f\"   • Total samples: {total_samples}\")\n",
    "    print(f\"   • Correct answers: {correct_count}\")\n",
    "    print(f\"   • Incorrect answers: {total_samples - correct_count}\")\n",
    "    print(f\"   • Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"📁 Updated dataset saved as: {output_file}\")\n",
    "    \n",
    "    # Show sample entries with the new column\n",
    "    print(f\"\\n📋 SAMPLE ENTRIES WITH 'is_correct' COLUMN:\")\n",
    "    for i, entry in enumerate(commonsenseqa_data[:3]):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"   ID: {entry['id']}\")\n",
    "        print(f\"   Concept: {entry['question_concept']}\")\n",
    "        print(f\"   Question: {entry['question'][:80]}...\")\n",
    "        print(f\"   Generated Answer: {entry['ans']}\")\n",
    "        print(f\"   Gold Answer: {entry['gold']}\")\n",
    "        print(f\"   Is Correct: {entry['is_correct']}\")\n",
    "    \n",
    "    # Also update the original file (backup approach)\n",
    "    backup_file = \"cot_commonsenseqa_final_1000_backup.json\"\n",
    "    if not os.path.exists(backup_file):\n",
    "        # Create backup of original\n",
    "        with open(input_file, 'r') as f:\n",
    "            original_data = json.load(f)\n",
    "        with open(backup_file, 'w') as f:\n",
    "            json.dump(original_data, f, indent=2)\n",
    "        print(f\"💾 Backup created: {backup_file}\")\n",
    "    \n",
    "    # Overwrite original file with updated data\n",
    "    with open(input_file, 'w') as f:\n",
    "        json.dump(commonsenseqa_data, f, indent=2)\n",
    "    print(f\"🔄 Original file updated: {input_file}\")\n",
    "    \n",
    "    # Show answer distribution analysis\n",
    "    print(f\"\\n📊 ANSWER DISTRIBUTION ANALYSIS:\")\n",
    "    answer_dist = {}\n",
    "    for entry in commonsenseqa_data:\n",
    "        ans = entry['ans']\n",
    "        answer_dist[ans] = answer_dist.get(ans, 0) + 1\n",
    "    print(f\"   Generated answers: {answer_dist}\")\n",
    "    \n",
    "    # Show accuracy by concept (sample analysis)\n",
    "    concept_accuracy = {}\n",
    "    for entry in commonsenseqa_data[:100]:  # Sample first 100 for concept analysis\n",
    "        concept = entry['question_concept']\n",
    "        if concept not in concept_accuracy:\n",
    "            concept_accuracy[concept] = {'correct': 0, 'total': 0}\n",
    "        concept_accuracy[concept]['total'] += 1\n",
    "        if entry['is_correct']:\n",
    "            concept_accuracy[concept]['correct'] += 1\n",
    "    \n",
    "    print(f\"\\n🎯 CONCEPT ACCURACY ANALYSIS (First 100 samples):\")\n",
    "    for concept, stats in list(concept_accuracy.items())[:5]:\n",
    "        acc = (stats['correct'] / stats['total']) * 100\n",
    "        print(f\"   {concept}: {acc:.1f}% ({stats['correct']}/{stats['total']})\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ File not found: {input_file}\")\n",
    "    print(\"   Make sure the CommonSenseQA final dataset file exists in the current directory\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
